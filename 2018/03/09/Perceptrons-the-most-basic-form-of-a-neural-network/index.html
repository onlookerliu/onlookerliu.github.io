<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Perceptrons - the most basic form of a neural network"><meta name="keywords" content="perceptron,machine learning,deep learning"><meta name="author" content="Monad Kai,undefined"><meta name="copyright" content="Monad Kai"><title>Perceptrons - the most basic form of a neural network | Code@浮生记</title><link rel="shortcut icon" href="https://avatars1.githubusercontent.com/u/168751?v=3&s=140"><link rel="stylesheet" href="../../../../css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.3"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: undefined
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Artificial-neural-networks-as-a-model-of-the-human-brain"><span class="toc-number">1.</span> <span class="toc-text">Artificial neural networks as a model of the human brain</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#What-can-neural-networks-do"><span class="toc-number">2.</span> <span class="toc-text">What can neural networks do?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-topology-of-a-neural-network"><span class="toc-number">3.</span> <span class="toc-text">The topology of a neural network</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neural-networks-must-learn"><span class="toc-number">4.</span> <span class="toc-text">Neural networks must learn</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Supervised-learning"><span class="toc-number">4.1.</span> <span class="toc-text">Supervised learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Unsupervised-learning"><span class="toc-number">4.2.</span> <span class="toc-text">Unsupervised learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Reinforced-learning"><span class="toc-number">4.3.</span> <span class="toc-text">Reinforced learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Neurons-The-building-blocks-of-neural-networks"><span class="toc-number">5.</span> <span class="toc-text">Neurons: The building blocks of neural networks</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Biology-vs-technology"><span class="toc-number">5.1.</span> <span class="toc-text">Biology vs technology</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Inside-an-artificial-neuron"><span class="toc-number">5.2.</span> <span class="toc-text">Inside an artificial neuron</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1-Each-input-gets-scaled-up-or-down"><span class="toc-number">5.2.1.</span> <span class="toc-text">1. Each input gets scaled up or down</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2-All-signals-are-summed-up"><span class="toc-number">5.2.2.</span> <span class="toc-text">2. All signals are summed up</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3-Activation"><span class="toc-number">5.2.3.</span> <span class="toc-text">3. Activation</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-perceptron"><span class="toc-number">6.</span> <span class="toc-text">The perceptron</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Can-we-do-something-useful-with-a-single-perceptron"><span class="toc-number">7.</span> <span class="toc-text">Can we do something useful with a single perceptron?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#The-code-A-perceptron-for-classifying-points"><span class="toc-number">8.</span> <span class="toc-text">The code: A perceptron for classifying points</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Exercises"><span class="toc-number">9.</span> <span class="toc-text">Exercises</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://oxyywddt8.bkt.clouddn.com/portrait/portrait.jpg"></div><div class="author-info__name text-center">Monad Kai</div><div class="author-info__description text-center">Life is beautiful!</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="../../../../archives"><span class="pull-left">文章</span><span class="pull-right">85</span></a><a class="author-info-articles__tags article-meta" href="../../../../tags"><span class="pull-left">标签</span><span class="pull-right">21</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://Lstyle1.github.io" target="_blank">Lyn</a><a class="author-info-links__name text-center" href="http://onlookerliu.leanote.com" target="_blank">另外一个关于数学的博客</a><a class="author-info-links__name text-center" href="http://jamesmwh.cn/" target="_blank">James MWH的博客</a><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的网站</a><a class="author-info-links__name text-center" href="http://www.ruanyifeng.com" target="_blank">阮一峰的网络日志</a><a class="author-info-links__name text-center" href="https://www.open-open.com" target="_blank">深度开源</a><a class="author-info-links__name text-center" href="https://www.nowcoder.com" target="_blank">牛客网</a><a class="author-info-links__name text-center" href="https://leetcode.com" target="_blank">LeetCode</a><a class="author-info-links__name text-center" href="http://www.learnyouahaskell.com" target="_blank">Haskell</a><a class="author-info-links__name text-center" href="https://www.kaggle.com/learn/overview" target="_blank">Kaggle</a><a class="author-info-links__name text-center" href="https://developers.google.cn/machine-learning/crash-course/framing/video-lecture" target="_blank">Google的AI课程</a><a class="author-info-links__name text-center" href="https://projecteuler.net/" target="_blank">Project Euler</a><a class="author-info-links__name text-center" href="https://beta.observablehq.com/" target="_blank">Observable Notebook</a><a class="author-info-links__name text-center" href="https://www.juliabox.com" target="_blank">JuliaBox</a><a class="author-info-links__name text-center" href="http://community.schemewiki.org/?scip-solutions" target="_blank">SICP Solutions</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://user-images.githubusercontent.com/12621342/37325500-23e8f77c-26c9-11e8-8e24-eb4346f1fff5.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Code@浮生记</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/gallery">Gallery</a></span></div><div id="post-info"><div id="post-title">Perceptrons - the most basic form of a neural network</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-03-09</time><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/03/09/Perceptrons-the-most-basic-form-of-a-neural-network/"></span></a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1,682</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><p>Artificial Neural Networks have gained attention during the recent years, driven by advances in deep learning. But what is an Artificial Neural Network and what is it made of?</p>
<p>Meet the perceptron.</p>
<p>In this article we’ll have a quick look at artificial neural networks in general, then we examine a single neuron, and finally (this is the coding part) we take the most basic version of an artificial neuron, the perceptron, and make it classify points on a plane.</p>
<p>But first, let me introduce the topic.</p>
<h3 id="Artificial-neural-networks-as-a-model-of-the-human-brain"><a href="#Artificial-neural-networks-as-a-model-of-the-human-brain" class="headerlink" title="Artificial neural networks as a model of the human brain"></a>Artificial neural networks as a model of the human brain</h3><p>Have you ever wondered why there are tasks that are dead simple for any human but incredibly difficult for computers? Artificial neural networks (short: ANN’s) were inspired by the central nervous system of humans. Like their biological counterpart, ANN’s are built upon simple signal processing elements that are connected together into a large mesh.</p>
<h3 id="What-can-neural-networks-do"><a href="#What-can-neural-networks-do" class="headerlink" title="What can neural networks do?"></a>What can neural networks do?</h3><p>ANN’s have been successfully applied to a number of problem domains:</p>
<ul>
<li>Classify data by recognizing patterns. Is this a tree on that picture?</li>
<li>Detect anomalies or novelties, when test data does not match the usual patterns. Is the truck driver at the risk of falling asleep? Are these seismic events showing normal ground motion or a big earthquake?</li>
<li>Process signals, for example, by filtering, separating, or compressing.</li>
<li>Approximate a target function–useful for predictions and forecasting. Will this storm turn into a tornado?</li>
</ul>
<p>Agreed, this sounds a bit abstract, so let’s look at some real-world applications. Neural networks can -</p>
<ul>
<li>identify faces,</li>
<li>recognize speech,</li>
<li>read your handwriting (mine perhaps not),</li>
<li>translate texts,</li>
<li>play games (typically board games or card games)</li>
<li>control autonomous vehicles and robots</li>
<li>and surely a couple more things!</li>
</ul>
<h3 id="The-topology-of-a-neural-network"><a href="#The-topology-of-a-neural-network" class="headerlink" title="The topology of a neural network"></a>The topology of a neural network</h3><p>There are many ways of knitting the nodes of a neural network together, and each way results in a more or less complex behavior. Possibly the simplest of all topologies is the feed-forward network. Signals flow in one direction only; there is never any loop in the signal paths.</p>
<div align="center"><br><img src="https://appliedgo.net/media/perceptron/ffnn.png"><br></div>

<p>Typically, ANN’s have a layered structure. The input layer picks up the input signals and passes them on to the next layer, the so-called ‘hidden’ layer. (Actually, there may be more than one hidden layer in a neural network.) Last comes the output layer that delivers the result.</p>
<h3 id="Neural-networks-must-learn"><a href="#Neural-networks-must-learn" class="headerlink" title="Neural networks must learn"></a>Neural networks must learn</h3><p>Unlike traditional algorithms, neural networks cannot be ‘programmed’ or ‘configured’ to work in the intended way. Just like human brains, they have to learn how to accomplish a task. Roughly speaking, there are three learning strategies:</p>
<h4 id="Supervised-learning"><a href="#Supervised-learning" class="headerlink" title="Supervised learning"></a>Supervised learning</h4><p>The easiest way. Can be used if a (large enough) set of test data with known results exists. Then the learning goes like this: Process one dataset. Compare the output against the known result. Adjust the network and repeat. This is the learning strategy we’ll use here.</p>
<h4 id="Unsupervised-learning"><a href="#Unsupervised-learning" class="headerlink" title="Unsupervised learning"></a>Unsupervised learning</h4><p>Useful if no test data is readily available, and if it is possible to derive some kind of cost function from the desired behavior. The cost function tells the neural network how much it is off the target. The network then can adjust its parameters on the fly while working on the real data.</p>
<h4 id="Reinforced-learning"><a href="#Reinforced-learning" class="headerlink" title="Reinforced learning"></a>Reinforced learning</h4><p>The ‘carrot and stick’ method. Can be used if the neural network generates continuous action. Follow the carrot in front of your nose! If you go the wrong way - ouch. Over time, the network learns to prefer the right kind of action and to avoid the wrong one.</p>
<p>Ok, now we know a bit about the nature of artificial neural networks, but what exactly are they made of? What do we see if we open the cover and peek inside?</p>
<h3 id="Neurons-The-building-blocks-of-neural-networks"><a href="#Neurons-The-building-blocks-of-neural-networks" class="headerlink" title="Neurons: The building blocks of neural networks"></a>Neurons: The building blocks of neural networks</h3><p>The very basic ingredient of any artificial neural network is the artificial neuron. They are not only named after their biological counterparts but also are modeled after the behavior of the neurons in our brain.</p>
<h4 id="Biology-vs-technology"><a href="#Biology-vs-technology" class="headerlink" title="Biology vs technology"></a>Biology vs technology</h4><p>Just like a biological neuron has dendrites to receive signals, a cell body to process them, and an axon to send signals out to other neurons, the artificial neuron has a number of input channels, a processing stage, and one output that can fan out to multiple other artificial neurons.</p>
<div align="center"><br><img src="https://appliedgo.net/media/perceptron/neuron.png"><br></div>

<h4 id="Inside-an-artificial-neuron"><a href="#Inside-an-artificial-neuron" class="headerlink" title="Inside an artificial neuron"></a>Inside an artificial neuron</h4><h5 id="1-Each-input-gets-scaled-up-or-down"><a href="#1-Each-input-gets-scaled-up-or-down" class="headerlink" title="1. Each input gets scaled up or down"></a>1. Each input gets scaled up or down</h5><p>When a signal comes in, it gets multiplied by a weight value that is assigned to this particular input. That is, if a neuron has three inputs, then it has three weights that can be adjusted individually. During the learning phase, the neural network can adjust the weights based on the error of the last test result.</p>
<h5 id="2-All-signals-are-summed-up"><a href="#2-All-signals-are-summed-up" class="headerlink" title="2. All signals are summed up"></a>2. All signals are summed up</h5><p>In the next step, the modified input signals are summed up to a single value. In this step, an offset is also added to the sum. This offset is called bias. The neural network also adjusts the bias during the learning phase.</p>
<p>This is where the magic happens! At the start, all the neurons have random weights and random biases. After each learning iteration, weights and biases are gradually shifted so that the next result is a bit closer to the desired output. This way, the neural network gradually moves towards a state where the desired patterns are “learned”.</p>
<h5 id="3-Activation"><a href="#3-Activation" class="headerlink" title="3. Activation"></a>3. Activation</h5><p>Finally, the result of the neuron’s calculation is turned into an output signal. This is done by feeding the result to an activation function (also called transfer function).</p>
<h3 id="The-perceptron"><a href="#The-perceptron" class="headerlink" title="The perceptron"></a>The perceptron</h3><p>The most basic form of an activation function is a simple binary function that has only two possible results.</p>
<div align="center"><br><img src="https://appliedgo.net/media/perceptron/heaviside.png"><br></div>

<p>Despite looking so simple, the function has a quite elaborate name: The Heaviside Step function. This function returns 1 if the input is positive or zero, and 0 for any negative input. A neuron whose activation function is a function like this is called a perceptron.</p>
<h3 id="Can-we-do-something-useful-with-a-single-perceptron"><a href="#Can-we-do-something-useful-with-a-single-perceptron" class="headerlink" title="Can we do something useful with a single perceptron?"></a>Can we do something useful with a single perceptron?</h3><p>If you think about it, it looks as if the perceptron consumes a lot of information for very little output - just 0 or 1. How could this ever be useful on its own?</p>
<p>There is indeed a class of problems that a single perceptron can solve. Consider the input vector as the coordinates of a point. For a vector with n elements, this point would live in an n-dimensional space. To make life (and the code below) easier, let’s assume a two-dimensional plane. Like a sheet of paper.</p>
<p>Further consider that we draw a number of random points on this plane, and we separate them into two sets by drawing a straight line across the paper:</p>
<div align="center"><br><img src="https://appliedgo.net/media/perceptron/pointsandline.png"><br></div>

<p>This line divides the points into two sets, one above and one below the line. (The two sets are then called linearly separable.)</p>
<p>A single perceptron, as bare and simple as it might appear, is able to learn where this line is, and when it finished learning, it can tell whether a given point is above or below that line.</p>
<p>Imagine that: A single perceptron already can learn how to classify points!</p>
<p>Let’s jump right into coding, to see how.</p>
<h3 id="The-code-A-perceptron-for-classifying-points"><a href="#The-code-A-perceptron-for-classifying-points" class="headerlink" title="The code: A perceptron for classifying points"></a>The code: A perceptron for classifying points</h3><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> main</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> (</span><br><span class="line">    <span class="string">"fmt"</span></span><br><span class="line">    <span class="string">"math/rand"</span></span><br><span class="line">    <span class="string">"time"</span></span><br><span class="line">    <span class="string">"github.com/appliedgo/perceptron/draw"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">type</span> Perceptron <span class="keyword">struct</span> &#123;</span><br><span class="line">    weights []<span class="keyword">float32</span></span><br><span class="line">    bias <span class="keyword">float32</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Perceptron)</span> <span class="title">heaviside</span><span class="params">(f <span class="keyword">float32</span>)</span> <span class="title">int32</span></span> &#123;</span><br><span class="line">    <span class="keyword">if</span> f &lt; <span class="number">0</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">NewPerceptron</span><span class="params">(n <span class="keyword">int32</span>)</span> *<span class="title">Perceptron</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> i <span class="keyword">int32</span></span><br><span class="line">    w := <span class="built_in">make</span>([]<span class="keyword">float32</span>, n, n)</span><br><span class="line">    <span class="keyword">for</span> i = <span class="number">0</span>; i &lt; n; i++ &#123;</span><br><span class="line">        w[i] = rand.Float32()*<span class="number">2</span> - <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> &amp;Perceptron&#123;</span><br><span class="line">        weights: w,</span><br><span class="line">        bias: rand.Float32()*<span class="number">2</span> - <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Perceptron)</span> <span class="title">Process</span><span class="params">(inputs []<span class="keyword">int32</span>)</span> <span class="title">int32</span></span> &#123;</span><br><span class="line">    sum := p.bias</span><br><span class="line">    <span class="keyword">for</span> i, input := <span class="keyword">range</span> inputs &#123;</span><br><span class="line">        sum += <span class="keyword">float32</span>(input) * p.weights[i]</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> p.heaviside(sum)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(p *Perceptron)</span> <span class="title">Adjust</span><span class="params">(inputs []<span class="keyword">int32</span>, delta <span class="keyword">int32</span>, learningRate <span class="keyword">float32</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> i, input := <span class="keyword">range</span> inputs &#123;</span><br><span class="line">        p.weights[i] += <span class="keyword">float32</span>(input) * <span class="keyword">float32</span>(delta) * learningRate</span><br><span class="line">    &#125;</span><br><span class="line">    p.bias += <span class="keyword">float32</span>(delta) * learningRate</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> (</span><br><span class="line">    a, b <span class="keyword">int32</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">f</span><span class="params">(x <span class="keyword">int32</span>)</span> <span class="title">int32</span></span> &#123;</span><br><span class="line">    <span class="keyword">return</span> a*x + b</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">isAboveLine</span><span class="params">(point []<span class="keyword">int32</span>, f <span class="keyword">func</span>(<span class="keyword">int32</span>)</span> <span class="title">int32</span>) <span class="title">int32</span></span> &#123;</span><br><span class="line">    x := point[<span class="number">0</span>]</span><br><span class="line">    y := point[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">if</span> y &gt; f(x) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">train</span><span class="params">(p *Perceptron, iters <span class="keyword">int</span>, rate <span class="keyword">float32</span>)</span></span> &#123;</span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; iters; i++ &#123;</span><br><span class="line">        point := []<span class="keyword">int32</span>&#123;</span><br><span class="line">            rand.Int31n(<span class="number">201</span>) - <span class="number">101</span>,</span><br><span class="line">            rand.Int31n(<span class="number">201</span>) - <span class="number">101</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        actual := p.Process(point)</span><br><span class="line">        expected := isAboveLine(point, f)</span><br><span class="line">        delta := expected - actual</span><br><span class="line">        p.Adjust(point, delta, rate)</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">verify</span><span class="params">(p *Perceptron)</span> <span class="title">int32</span></span> &#123;</span><br><span class="line">    <span class="keyword">var</span> correctAnswers <span class="keyword">int32</span> = <span class="number">0</span></span><br><span class="line">    c := draw.NewCanvas()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i := <span class="number">0</span>; i &lt; <span class="number">100</span>; i++ &#123;</span><br><span class="line">        point := []<span class="keyword">int32</span>&#123;</span><br><span class="line">            rand.Int31n(<span class="number">201</span>) - <span class="number">101</span>,</span><br><span class="line">            rand.Int31n(<span class="number">201</span>) - <span class="number">101</span>,</span><br><span class="line">        &#125;</span><br><span class="line">        result := p.Process(point)</span><br><span class="line">        <span class="keyword">if</span> result == isAboveLine(point, f) &#123;</span><br><span class="line">            correctAnswer += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        c.DrawPoint(point[<span class="number">0</span>], point[<span class="number">1</span>], result == <span class="number">1</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    c.DrawLinearFunction(a, b)</span><br><span class="line">    c.Save()</span><br><span class="line">    <span class="keyword">return</span> correctAnswers</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">main</span><span class="params">()</span></span> &#123;</span><br><span class="line">    rand.Seed(time.Now().UnixNano())</span><br><span class="line">    a = rand.Int31n(<span class="number">11</span>) - <span class="number">6</span></span><br><span class="line">    b = rand.Int31n(<span class="number">101</span>) - <span class="number">51</span></span><br><span class="line">    p := NewPerceptron(<span class="number">2</span>)</span><br><span class="line">    iterations := <span class="number">1000</span></span><br><span class="line">    <span class="keyword">var</span> learningRate <span class="keyword">float32</span> = <span class="number">0.1</span></span><br><span class="line">    train(p, iterations, learningRate)</span><br><span class="line">    successRate := verify(p)</span><br><span class="line">    fmt.Printf(<span class="string">"%d%% of the answers were correct\n"</span>, successRate)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h3><ol>
<li><p>Play with the number of training iterations!</p>
<ul>
<li>Will the accuracy increase if you train the perceptron 10,000 times?</li>
<li>Try fewer iterations. What happens if you train the perceptron only 100 times? 10times?</li>
<li>What happens if you skip the training completely?</li>
</ul>
</li>
<li><p>Change the learning rate to 0.01, 0.2, 0.0001, 0.5, 1,… while keeping the training iterations constant. Do you see the accuracy change?</p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Monad Kai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="../../../../onlookerliu.github.io/2018/03/09/Perceptrons-the-most-basic-form-of-a-neural-network/">onlookerliu.github.io/2018/03/09/Perceptrons-the-most-basic-form-of-a-neural-network/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="onlookerliu.github.io" target="_blank">Code@浮生记</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../../tags/perceptron/">perceptron</a><a class="post-meta__tags" href="../../../../tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="../../../../tags/deep-learning/">deep learning</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="http://oxyywddt8.bkt.clouddn.com/qrcode/ali-qrcode.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="http://oxyywddt8.bkt.clouddn.com/qrcode/Wechat.jpeg"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="addthis_inline_share_toolbox pull-right"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=undefined" async></script><nav id="pagination"><div class="prev-post pull-left"><a href="../../10/Project-Euler-015/"><i class="fa fa-chevron-left">  </i><span>Project-Euler-015</span></a></div><div class="next-post pull-right"><a href="../Project-Euler-014/"><span>Project-Euler-014</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'onlookerliu.github.io/2018/03/09/Perceptrons-the-most-basic-form-of-a-neural-network/';
  this.page.identifier = '2018/03/09/Perceptrons-the-most-basic-form-of-a-neural-network/';
  this.page.title = 'Perceptrons - the most basic form of a neural network';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'onlookerliu' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://onlookerliu.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2018 By Monad Kai</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="../../../../js/third-party/anime.min.js"></script><script src="../../../../js/third-party/jquery.min.js"></script><script src="../../../../js/third-party/jquery.fancybox.min.js"></script><script src="../../../../js/third-party/velocity.min.js"></script><script src="../../../../js/third-party/velocity.ui.min.js"></script><script src="../../../../js/utils.js?version=1.5.3"></script><script src="../../../../js/fancybox.js?version=1.5.3"></script><script src="../../../../js/sidebar.js?version=1.5.3"></script><script src="../../../../js/copy.js?version=1.5.3"></script><script src="../../../../js/fireworks.js?version=1.5.3"></script><script src="../../../../js/transition.js?version=1.5.3"></script><script src="../../../../js/scroll.js?version=1.5.3"></script><script src="../../../../js/head.js?version=1.5.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>