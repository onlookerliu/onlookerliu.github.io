<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="ML-Lectures Perceptron"><meta name="keywords" content="machine learning,deep learning,python"><meta name="author" content="Monad Kai,undefined"><meta name="copyright" content="Monad Kai"><title>ML-Lectures Perceptron | Code@浮生记</title><link rel="shortcut icon" href="https://avatars1.githubusercontent.com/u/168751?v=3&s=140"><link rel="stylesheet" href="../../../../css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.5.3"><link rel="dns-prefetch" href="https://cdn.staticfile.org"><link rel="dns-prefetch" href="https://cdn.bootcss.com"><link rel="dns-prefetch" href="https://creativecommons.org"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: undefined
} </script></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#从感知机到简单神经网络"><span class="toc-number">1.</span> <span class="toc-text">从感知机到简单神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-什么是感知机？"><span class="toc-number">1.1.</span> <span class="toc-text">0. 什么是感知机？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-感知机模型"><span class="toc-number">1.2.</span> <span class="toc-text">1. 感知机模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-感知机学习策略"><span class="toc-number">1.3.</span> <span class="toc-text">2. 感知机学习策略</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-感知机学习算法"><span class="toc-number">1.4.</span> <span class="toc-text">3. 感知机学习算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-感知机学习算法的原始形式"><span class="toc-number">1.4.1.</span> <span class="toc-text">3.1 感知机学习算法的原始形式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Algorithm-1"><span class="toc-number">1.4.1.1.</span> <span class="toc-text">Algorithm 1:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-算法收敛性"><span class="toc-number">1.4.2.</span> <span class="toc-text">3.2 算法收敛性</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-感知机学习算法的对偶形式"><span class="toc-number">1.4.3.</span> <span class="toc-text">3.3 感知机学习算法的对偶形式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Algorithm-2"><span class="toc-number">1.4.3.1.</span> <span class="toc-text">Algorithm 2:</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-感知机学习算法应用实例"><span class="toc-number">1.5.</span> <span class="toc-text">4. 感知机学习算法应用实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-鸢尾花数据集分类"><span class="toc-number">1.5.1.</span> <span class="toc-text">4.1. 鸢尾花数据集分类</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#python源码实现感知机"><span class="toc-number">1.5.1.1.</span> <span class="toc-text">python源码实现感知机</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#利用sklearn实现感知机方法"><span class="toc-number">1.5.1.2.</span> <span class="toc-text">利用sklearn实现感知机方法</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-从感知机到人工神经网络ANN"><span class="toc-number">1.5.2.</span> <span class="toc-text">4.2. 从感知机到人工神经网络ANN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-单层感知机求解AND-OR问题"><span class="toc-number">1.5.3.</span> <span class="toc-text">4.3. 单层感知机求解AND/OR问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#原始解法"><span class="toc-number">1.5.3.1.</span> <span class="toc-text">原始解法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#利用TensorFlow求解single-perceptron"><span class="toc-number">1.5.3.2.</span> <span class="toc-text">利用TensorFlow求解single-perceptron</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#多层感知机求解XOR问题"><span class="toc-number">1.5.3.3.</span> <span class="toc-text">多层感知机求解XOR问题</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-利用多层感知机训练MNIST数据集"><span class="toc-number">1.5.4.</span> <span class="toc-text">4.4. 利用多层感知机训练MNIST数据集</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Exercise"><span class="toc-number">1.6.</span> <span class="toc-text">5. Exercise:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Reference"><span class="toc-number">1.7.</span> <span class="toc-text">6. Reference</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://oxyywddt8.bkt.clouddn.com/portrait/portrait.jpg"></div><div class="author-info__name text-center">Monad Kai</div><div class="author-info__description text-center">Life is beautiful!</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="../../../../archives"><span class="pull-left">文章</span><span class="pull-right">85</span></a><a class="author-info-articles__tags article-meta" href="../../../../tags"><span class="pull-left">标签</span><span class="pull-right">21</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" href="https://Lstyle1.github.io" target="_blank">Lyn</a><a class="author-info-links__name text-center" href="http://onlookerliu.leanote.com" target="_blank">另外一个关于数学的博客</a><a class="author-info-links__name text-center" href="http://jamesmwh.cn/" target="_blank">James MWH的博客</a><a class="author-info-links__name text-center" href="https://www.liaoxuefeng.com" target="_blank">廖雪峰的网站</a><a class="author-info-links__name text-center" href="http://www.ruanyifeng.com" target="_blank">阮一峰的网络日志</a><a class="author-info-links__name text-center" href="https://www.open-open.com" target="_blank">深度开源</a><a class="author-info-links__name text-center" href="https://www.nowcoder.com" target="_blank">牛客网</a><a class="author-info-links__name text-center" href="https://leetcode.com" target="_blank">LeetCode</a><a class="author-info-links__name text-center" href="http://www.learnyouahaskell.com" target="_blank">Haskell</a><a class="author-info-links__name text-center" href="https://www.kaggle.com/learn/overview" target="_blank">Kaggle</a><a class="author-info-links__name text-center" href="https://developers.google.cn/machine-learning/crash-course/framing/video-lecture" target="_blank">Google的AI课程</a><a class="author-info-links__name text-center" href="https://projecteuler.net/" target="_blank">Project Euler</a><a class="author-info-links__name text-center" href="https://beta.observablehq.com/" target="_blank">Observable Notebook</a><a class="author-info-links__name text-center" href="https://www.juliabox.com" target="_blank">JuliaBox</a><a class="author-info-links__name text-center" href="http://community.schemewiki.org/?scip-solutions" target="_blank">SICP Solutions</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://user-images.githubusercontent.com/12621342/37325500-23e8f77c-26c9-11e8-8e24-eb4346f1fff5.png)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Code@浮生记</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/gallery">Gallery</a></span></div><div id="post-info"><div id="post-title">ML-Lectures Perceptron</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2018-04-13</time><span class="post-meta__separator">|</span><i class="fa fa-comment-o post-meta__icon" aria-hidden="true"></i><a href="#disqus_thread"><span class="disqus-comment-count" data-disqus-identifier="2018/04/13/ML-Lectures-Perceptron/"></span></a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">18,053</span><span class="post-meta__separator">|</span><span>阅读时长: 75 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><h1 id="从感知机到简单神经网络"><a href="#从感知机到简单神经网络" class="headerlink" title="从感知机到简单神经网络"></a>从感知机到简单神经网络</h1><p>近年来，人工神经网络在深度学习的推动下获得了关注。什么是人造神经网络，它是由什么构成的？我想我们可能需要从感知器开始学起。</p>
<p>在这个讲座中，我们将严格推导感知机学习算法及其对偶理论，掌握一般的人工神经网络，并对单层甚至多层感知器进行编码实现。在几个实战例子中，我们采用人工神经元的最基本版本——感知器，来在超平面上对我们的数据集进行分类。</p>
<a id="more"></a>
<h2 id="0-什么是感知机？"><a href="#0-什么是感知机？" class="headerlink" title="0. 什么是感知机？"></a>0. 什么是感知机？</h2><p>感知机是<strong>二分类</strong>的线性分类模型，输入为实例的特征向量，输出为实例的类别（取+1和-1）。感知机对应于输入空间中<strong>将实例划分为两类</strong>的分离超平面。感知机属于判别模型(classifier)。</p>
<p>感知机旨在求出该超平面，为求得超平面导入了<strong>基于误分类的损失函数</strong>，利用<strong>梯度下降法</strong>对损失函数进行最优化。</p>
<h2 id="1-感知机模型"><a href="#1-感知机模型" class="headerlink" title="1. 感知机模型"></a>1. 感知机模型</h2><p>假设输入空间（特征向量）是 $\mathcal{X} \subseteq \mathbb{R}^n$，输出空间为 $\mathcal{Y} = {-1，+1}$. 输入 $X$ 表示实例的特征向量，对应于输入空间的点，输出 $y\in\mathcal{Y}$ 表示实例的类别，则由输入空间到输出空间的表达形式为：</p>
<p>$$<br>f(x) = \text{sign}(w \cdot x + b)<br>$$</p>
<p>其中 $w,b$ 称为模型的参数，$w$ 称为权值，$b$ 称为偏置，$ w\cdot x$表示为 $w,x$ 的内积. <code>sign</code>是符号函数</p>
<p>$$<br>\text{sign}(x) = \begin{cases}<br>+1 &amp; x &gt; 0 \<br>-1 &amp; x &lt; 0<br>\end{cases}<br>$$</p>
<p>如果我们将<code>sign</code>称之为激活函数的话，感知机与logistic regression的差别就是感知机激活函数是sign，logistic regression的激活函数是sigmoid.</p>
<p>$\text{sign}(x)​$ 将大于0的分为1，小于0的分为-1；sigmoid将大于0.5的分为1，小于0.5的分为0。因此sign又被称为单位阶跃函数，logistic regression也被看作是一种概率估计。(logistic后面会详细讲解)</p>
<p>感知机是一种线性分类模型，属于判别模型。感知机模型的假设空间是定义在特征空间中的所有线性分类模型(linear classification model)或线性分类器(linear classifier)，即函数集合 ${ f \mid f(x) = w \cdot x + b }$.</p>
<p>由感知机的线性方程表示 $w \cdot x + b = 0$ 可以看出它的几何意义：</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/perceptron-2.1.png" width="300"><br></div>

<p>对于特征空间 $\mathbb{R}^n$ 中的一个超平面 $S$ ，其中 $w$ 是超平面的法向量，$b$是超平面的截距。这个超平面 (seperating hyperplane) 将特征空间分为两部分，位于两部分的点(特征向量)分别被分为正、负两类。</p>
<p>我们其实就是在<strong>学习参数 $w$ 与 $b$</strong>，确定了 $w$ 与 $b$，图上的直线（高维空间下为超平面）也就确定了，那么以后来一个数据点，我们用训练好的模型进行预测判断，如果大于 $0$ 就分类到 $+1$，如果小于 $0$ 就分类到 $-1$。</p>
<p>能这么做的原因其实是<a href="https://en.wikipedia.org/wiki/Hyperplane_separation_theorem" target="_blank" rel="noopener">超平面分离定理</a>：超平面分离定理是应用凸集到最优化理论中的重要结果，这个结果在最优化理论中有重要的位置。所谓两个凸集分离，直观地看是指两个凸集合没有交叉和重合的部分，因此<strong>可以用一张超平面将两者隔在两边</strong>。</p>
<p>如下图所示，在大于0的时候，我将数据点分类成了D类，在小于0的时候，我将数据点分类成了C类</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/seperating.png" width="300"><br></div>

<h2 id="2-感知机学习策略"><a href="#2-感知机学习策略" class="headerlink" title="2. 感知机学习策略"></a>2. 感知机学习策略</h2><p>好了，上面我们已经知道感知机模型了，我们也知道他的任务是解决二分类问题，也知道了超平面的形式，那么下面关键是如何学习出超平面的参数 $w,b$，这就需要用到我们的学习策略。</p>
<p>我们知道机器学习模型，需要首先找到损失函数，然后转化为最优化问题，用梯度下降等方法进行更新，最终学习到我们模型的参数 $w,b$。</p>
<p>Ok, 我们开始来找感知机的损失函数:</p>
<p>我们很自然的会想到用误分类点的数目来作为损失函数，是的误分类点个数越来越少嘛，感知机本来也是做这种事的，只需要全部分对就好。</p>
<p>但是不幸的是，这样的损失函数并不是w，b连续可导（你根本就无法用函数形式来表达出误分类点的个数），无法进行优化。</p>
<p>于是我们想转为另一种选择，误分类点到超平面的总距离（直观来看，总距离越小越好）,距离公式如下</p>
<p>$$<br>\frac{1}{\Vert w \Vert} \vert w\cdot x_0 + b\vert<br>$$</p>
<p>而我们知道每一个<strong>误分类点</strong>都满足</p>
<p>$$<br>-y_i(w\cdot x_i + b) &gt; 0<br>$$</p>
<p>因为当我们数据点正确值为 $+1$ 的时候，你误分类了，那么你判断为 $-1$，则算出来$(w\cdot x_i+b)&lt;0$，可以将绝对值符号去掉，得到误分类点的距离为：</p>
<p>$$<br>-\frac{1}{\Vert w\Vert} y_i(w\cdot x_i + b)<br>$$</p>
<p>这样的话，假设超平面 $S$ 的误分类点集合为 $M$，那么所有误分类点到超平面 $S$ 的总距离为</p>
<p>$$<br>-\frac{1}{\Vert w\Vert} \sum_{x_i\in M} y_i(w\cdot x_i + b)<br>$$</p>
<p>不考虑 $\dfrac{1}{\Vert w\Vert}$，就得到了感知机学习的损失函数</p>
<p>$$<br>L(w, b) = -\sum_{x_i\in M} y_i(w\cdot x_i + b)<br>$$</p>
<p>其中 $M$ 为误分类点的数目，这个损失函数就是感知机学习的经验风险函数。</p>
<p><em>Question:</em><br>为什么可以不考虑 $\frac{1}{\Vert w\Vert}$，不用总距离表达式作为损失函数呢？</p>
<p><em>Answer:</em><br>感知机的任务是进行二分类工作，它最终并不关心得到的超平面离各点的距离有多少（所以我们最后才可以不考虑$\Vert w\Vert$），只是关心我最后是否已经正确分类正确（也就是考虑误分类点的个数），比如说下面红色与绿线，对于感知机来说，效果任务是一样好的。</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/640.png" width="250"><br></div>

<p>但是在SVM的评价标准中，绿线是要比红线好的。对比书中SVM示意图</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/svm.png" width="250"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/large_margin_intuition.jpg" width="300"><br></div>

<p>考虑这样右边的分类问题</p>
<p>可以看出</p>
<ul>
<li>感知机追求最大程度正确划分，最小化错误，效果类似紫线，很容易造成过拟合。</li>
<li>支持向量机追求在大致正确分类的同时，最大化margin，一定程度上避免了过拟合，效果类似黑线。margin可以理解为黑线到圈类和叉类之间的最短距离。</li>
<li>去掉距离限制的SVM，就是一种PLA，当然先不考虑核。</li>
</ul>
<p>这里我们可以不考虑 $\Vert w\Vert$，直接去掉它，因为这个时候我们只考虑误分类点，当一个误分类点出现的时候，我们进行梯度下降，对 $w,b$ 进行改变即可！</p>
<p>跟距离没有什么关系了，因为 $w$ 的范数始终是大于0，对于我们判断是否为误分类点（我们是通过 $-y_i(w\cdot x_i+b)&gt;0$ 来判断是否为误分类点）没有影响！</p>
<p>这也回到了我们最初始想要作为损失函数的误分类点的个数，<u>引入距离，只是将它推导出一个可导的形式！</u></p>
<p>最后说一句，我个人认为不去掉 $\Vert w\Vert$，也是一样可以得到最后的正确分类超平面，就是直接用距离来当做损失函数也是可以的，可能是求梯度比较复杂，或者是感知机本身就是用误分类点来区分，就没用这个损失函数了。</p>
<p>根据知乎吴洋文章中所做的实验，不考虑$\Vert w\Vert$ 的时候，结果如下:</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/noNorm.png" width="400"><br></div>

<p>考虑$\Vert w\Vert$ 的时候，结果如下:</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/hasNorm.png" width="400"><br></div>

<p>可以看到，无论是否考虑，实验收敛次数并没有改变！</p>
<p>那么好了，我们已经得到了损失函数了，后面直接讲解如何梯度下降，收敛到分类正确为止。</p>
<h2 id="3-感知机学习算法"><a href="#3-感知机学习算法" class="headerlink" title="3. 感知机学习算法"></a>3. 感知机学习算法</h2><h3 id="3-1-感知机学习算法的原始形式"><a href="#3-1-感知机学习算法的原始形式" class="headerlink" title="3.1 感知机学习算法的原始形式"></a>3.1 感知机学习算法的原始形式</h3><p>当我们已经有了一个目标是最小化损失函数，我们就可以用常用的梯度下降方法来进行更新，对w，b参数分别进行求偏导可得：</p>
<p>$$<br>\begin{split}<br>&amp;\nabla<em>w L(w,b) = -\sum</em>{x_i\in M} y_ix_i \<br>&amp;\nabla<em>b L(w,b) = -\sum</em>{x_i\in M} y_i<br>\end{split}<br>$$</p>
<p>那么我们任意初始化 $w, b$ 之后，碰到误分类点时，采取的权值更新为 $w,b$分别为：</p>
<p>$$<br>\begin{split}<br>&amp;w = w + \sum_{x_i\in M} y_ix<em>i \<br>&amp;b = b + \sum</em>{x_i\in M} y_i<br>\end{split}<br>$$</p>
<p>好了，当我们碰到误分类点的时候，我们就采取上面的更新步骤进行更新参数即可！但李航博士在书中并不是用到所有误分类点的数据点来进行更新，而是采取==随机梯度下降法==（stochastic gradient descent）。</p>
<p>步骤如下，首先，任取一个超平面 $w_0, b0$，然后用梯度下降法不断地极小化目标函数，极小化过程中<strong>不是一次使M中所有误分类点的梯度下降</strong>而是<strong>一次随机选取一个误分类点使其梯度下降</strong></p>
<p><strong>Remark:</strong><br>有证明可以证明随机梯度下降可以收敛，并且收敛速度快于批量梯度下降，在这里不是我们考虑的重点，我们默认为它能收敛到最优点即可</p>
<p>那么碰到误分类点的时候，采取的权值更新 $w, b$ 分别为:</p>
<p>$$<br>\begin{split}<br>&amp;w = w + \eta\, y_ix_i \<br>&amp;b = b + \eta\, y_i<br>\end{split}<br>$$</p>
<p>好了，至此我们可以给出整个感知机学习过程算法！如下：</p>
<h4 id="Algorithm-1"><a href="#Algorithm-1" class="headerlink" title="Algorithm 1:"></a><strong>Algorithm 1:</strong></h4><ol>
<li>选定初值 $w_0,b_0$，相当于初始给了一个超平面</li>
<li>在训练集中选取数据 $(x_i, y_i)$ (任意抽取数据点，判断是否所有数据点判断完成没有误分类点了，如果没有了，直接结束算法，如果还有进入3.)</li>
<li>若 $y_i(w \cdot x_i + b) \leq 0$，说明是误分类点，需要进行参数更新，更新方式如下<br>$$<br>\begin{split}<br>&amp;w = w + \eta\, y_ix_i \<br>&amp;b = b + \eta\, y_i<br>\end{split}<br>$$</li>
<li>转到2.，直到训练集中没有误分类点</li>
</ol>
<p>对于第三步的更新方式，我们有一个直观上的感觉，可视化如下图</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/update-1.png" width="150"><br></div>

<p>当我们数据点应该分类为 $y=+1$ 的时候，我们分错了，分成 $-1$，说明 $w\cdot x&lt;0$，代表 $w$ 与 $x$向量夹角大于90度，这个时候应该调整，更新过程为 $w=w+1\cdot x$，往$x$ 向量方向更接近了。</p>
<p>另一种更新情形如下图</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/update-2.png" width="150"><br></div>

<p>当我们数据点应该分类为 $y=-1$ 的时候，我们分错了，分成 $+1$，说明 $w\cdot x&gt;0$，代表 $w$ 与 $x$ 向量夹角小于90度，这个时候应该调整，更新过程为 $w=w-1\cdot x$，往远离 $x$ 向量方向更接近了。</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/tune.png" width="600"><br></div>

<p><strong>Example:</strong><br>如下图所示的训练数据集，其正实例点是 $x_1 = (3, 3)^T, x_2 = (4, 3)^T$，负实例点是 $x_3 = (1,1)^T$，用感知机学习方法的原始形式求感知机模型 $f(x) = \text{sign}(w\cdot x + b)$。记 $w = (w^{(1)}, w^{(2)})^T, x = (x^{(1)}, x^{(2)})^T$</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/perceptron-2.2.png" width="400"><br></div>

<p><strong>Solution:</strong><br>构建优化问题:</p>
<p>$$<br>\min<em>{w,b} L(w,b) = -\sum</em>{x_i\in M} y_i(w\cdot x_i + b)<br>$$</p>
<p>求解：$w, b, \eta=1$</p>
<ol>
<li>取初值 $w_0 = 0, b_0 = 0$</li>
<li>对 $x_1=(3,3)^T, y_1(w_0\cdot x_1 + b_0)=0$，未能被正常分类，更新 $w,b$<br>$$<br>w_1 = w_0+y_1x_1=(3,3)^T \qquad b_1=b_0+y_1=1<br>$$<br>得线性模型<br>$$<br>w_1\cdot x+b_1 = 3x^{(1)}+3x^{(2)}+1<br>$$</li>
<li>对 $x_1, x_2$，显然，$y_i(w_1\cdot x_i+b_1)&gt;0$，被正确分类，不修改 $w,b$:<br>对 $x_3=(1,1)^T, y_3(w_1\cdot x_3+b_1)&lt;0$，被误分类，更新 $w,b$.</li>
</ol>
<p>$$<br>w_2=w_1+y_3x_3=(2,2)^T \qquad b_2=b_1+y_3=0<br>$$</p>
<p>得到线性模型</p>
<p>$$<br>w_2\cdot x+b_2=2x^{(1)}+2x^{(2)}<br>$$</p>
<p>如此继续下去，直到</p>
<p>$$<br>\begin{split}<br>&amp;w_7=(1,1)^T \quad b_7=-3 \<br>&amp;w_7\cdot x+b_7=x^{(1)}+x^{(2)}-3<br>\end{split}<br>$$</p>
<p>对于所有数据点 $y_i(w_7\cdot x_i+b_7)&gt;0$，没有误分类点，损失函数达到极小.</p>
<p>分离超平面为</p>
<p>$$<br>x^{(1)} + x^{(2)} - 3 = 0<br>$$</p>
<p>感知机模型为</p>
<p>$$<br>f(x) = \text{sign}(x^{(1)}+x^{(2)}-3)<br>$$</p>
<p>迭代过程如下表</p>
<table>
<thead>
<tr>
<th>迭代次数</th>
<th>误分类点</th>
<th>$w$</th>
<th>$b$</th>
<th>$w\cdot x+b$</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td></td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>1</td>
<td>$x_1$</td>
<td>$(3,3)^T$</td>
<td>1</td>
<td>$3x^{(1)}+3x^{(2)}+1$</td>
</tr>
<tr>
<td>2</td>
<td>$x_3$</td>
<td>$(2,2)^T$</td>
<td>0</td>
<td>$2x^{(1)}+2x^{(2)}$</td>
</tr>
<tr>
<td>3</td>
<td>$x_3$</td>
<td>$(1,1)^T$</td>
<td>-1</td>
<td>$x^{(1)}+x^{(2)}-1$</td>
</tr>
<tr>
<td>4</td>
<td>$x_3$</td>
<td>$(0,0)^T$</td>
<td>-2</td>
<td>-2</td>
</tr>
<tr>
<td>5</td>
<td>$x_1$</td>
<td>$(3,3)^T$</td>
<td>-1</td>
<td>$3x^{(1)}+3x^{(2)}-1$</td>
</tr>
<tr>
<td>6</td>
<td>$x_3$</td>
<td>$(2,2)^T$</td>
<td>-2</td>
<td>$2x^{(1)}+2x^{(2)}-2$</td>
</tr>
<tr>
<td>7</td>
<td>$x_3$</td>
<td>$(1,1)^T$</td>
<td>-3</td>
<td>$x^{(1)}+x^{(2)}-3$</td>
</tr>
<tr>
<td>8</td>
<td>0</td>
<td>$(1,1)^T$</td>
<td>-3</td>
<td>$x^{(1)}+x^{(2)}-3$</td>
</tr>
</tbody>
</table>
<p>根据上述例子和算法过程，可以用如下python代码实现计算。</p>
<p>核心算法流程图如下</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/alg.png" height="250"><br></div>

<ul>
<li><p>初始化训练数据以及参数 $w,b$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_set = [[(<span class="number">3</span>,<span class="number">3</span>), <span class="number">1</span>], [(<span class="number">4</span>,<span class="number">3</span>), <span class="number">1</span>], [(<span class="number">1</span>,<span class="number">1</span>), <span class="number">-1</span>]]</span><br><span class="line">w = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">max_iter = <span class="number">1000</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>main函数逻辑，如果在<code>max_iter</code>步之内全部正确分类给出提示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    all_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> misclassified():</span><br><span class="line">            all_correct = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> all_correct:</span><br><span class="line">        print(<span class="string">"All points are correctly classified within max iterations!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Still not enough."</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>product()</code>函数用于计算 $y_i(w\cdot x_i+b)$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span><span class="params">(item)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(item[<span class="number">0</span>])):</span><br><span class="line">        sum += item[<span class="number">0</span>][i] * w[i]</span><br><span class="line">    sum += b</span><br><span class="line">    sum *= item[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>misclassified()</code>函数用于判断是否误分类</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">misclassified</span><span class="params">()</span>:</span></span><br><span class="line">    is_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> train_set:</span><br><span class="line">        <span class="keyword">if</span> product(item) &lt;= <span class="number">0</span>:</span><br><span class="line">            is_correct = <span class="keyword">True</span></span><br><span class="line">            update(item)</span><br><span class="line">    <span class="keyword">return</span> is_correct</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>update()</code>函数用于确定误分类之后的一步更新操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> w, b</span><br><span class="line">    w[<span class="number">0</span>] += <span class="number">1</span>*item[<span class="number">1</span>]*item[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    w[<span class="number">1</span>] += <span class="number">1</span>*item[<span class="number">1</span>]*item[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    b += <span class="number">1</span>*item[<span class="number">1</span>]</span><br><span class="line">    print(<span class="string">"w = "</span>, w, <span class="string">"b = "</span>, b)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>总的程序代码如下：<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">train_set = [[(<span class="number">3</span>,<span class="number">3</span>), <span class="number">1</span>], [(<span class="number">4</span>,<span class="number">3</span>), <span class="number">1</span>], [(<span class="number">1</span>,<span class="number">1</span>), <span class="number">-1</span>]]</span><br><span class="line">w = [<span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">b = <span class="number">0</span></span><br><span class="line">max_iter = <span class="number">1000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span><span class="params">(item)</span>:</span></span><br><span class="line">    sum = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(item[<span class="number">0</span>])):</span><br><span class="line">        sum += item[<span class="number">0</span>][i] * w[i]</span><br><span class="line">    sum += b</span><br><span class="line">    sum *= item[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> sum</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">misclassified</span><span class="params">()</span>:</span></span><br><span class="line">    is_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> train_set:</span><br><span class="line">        <span class="keyword">if</span> product(item) &lt;= <span class="number">0</span>:</span><br><span class="line">            is_correct = <span class="keyword">True</span></span><br><span class="line">            update(item)</span><br><span class="line">    <span class="keyword">return</span> is_correct</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(item)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> w, b</span><br><span class="line">    w[<span class="number">0</span>] += <span class="number">1</span>*item[<span class="number">1</span>]*item[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line">    w[<span class="number">1</span>] += <span class="number">1</span>*item[<span class="number">1</span>]*item[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">    b += <span class="number">1</span>*item[<span class="number">1</span>]</span><br><span class="line">    print(<span class="string">"w = "</span>, w, <span class="string">"b = "</span>, b)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    all_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> misclassified():</span><br><span class="line">            all_correct = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> all_correct:</span><br><span class="line">        print(<span class="string">"All points are correctly classified within max iterations!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"Still not enough."</span>)</span><br></pre></td></tr></table></figure></p>
<p>程序运行结果如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w =  [3, 3] b =  1</span><br><span class="line">w =  [2, 2] b =  0</span><br><span class="line">w =  [1, 1] b =  -1</span><br><span class="line">w =  [0, 0] b =  -2</span><br><span class="line">w =  [3, 3] b =  -1</span><br><span class="line">w =  [2, 2] b =  -2</span><br><span class="line">w =  [1, 1] b =  -3</span><br><span class="line">All points are correctly classified within max iterations!</span><br></pre></td></tr></table></figure></p>
<p>这与我们书本上的结果是对应的。</p>
<p><strong>Remark:</strong></p>
<ol>
<li><p>上述结果是在误分类点先后取 $x_1, x_3, x_3, x_3, x_1, x_3, x_3$ 得到的分离超平面和感知机模型。如果在计算中误分类点依次取 $x_1, x_3, x_3, x_3, x_2, x_3, x_3, x_3, x_1, x_3, x_3$，那么得到的分离超平面是 $2x^{(1)}+x^{(2)}-5=0$.</p>
</li>
<li><p>可以发现，感知机方法对于不同初值或取不同的误分类点，解可以不同。</p>
</li>
<li><p>与SVM不同之处，感知机只能做到产生一个分割，但并不能产生一个很好的分割。对于分类问题，感知机是可用的，但如果用来预测新样本的属性，最好还是用SVM。</p>
</li>
</ol>
<p>至此，感知机学习算法以及简单的python实现已经讲完了，下面讲解一下感知机的对偶形式，以及证明一下感知机学习算法为什么在迭代有限次的时候可以收敛。</p>
<h3 id="3-2-算法收敛性"><a href="#3-2-算法收敛性" class="headerlink" title="3.2 算法收敛性"></a>3.2 算法收敛性</h3><p>现在证明，对于线性可分数据集，感知机方法原始形式收敛。也就是说，经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面和感知机模型。</p>
<p>为了方便推导，将bias偏置 $b$ 并入权重向量 $w$ 中，记作 $\hat w = (w^T, b)^T$。这需要同时将输入向量加以扩充，补入常数 $1$，记作 $\hat x = (x^T, 1)^T$。这样，$\hat x\in\mathbb{R}^{n+1}, \hat w\in\mathbb{R}^{n+1}$，$\hat w\cdot \hat x = w\cdot x+b$。</p>
<p><strong>Novikoff’s Theorem:</strong><br>设训练数据集 $T = { (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N) }$ 是线性可分的，其中 $x_i\in\mathcal{X}=\mathbb{R}^n$，$y_i\in\mathcal{Y}={-1,+1}, i=1,2,\cdots,N$，则</p>
<ol>
<li>存在满足条件 $\Vert \hat w<em>{\text{opt}}\Vert = 1$ 的超平面 $\hat w</em>{\text{opt}}\cdot \hat x = w<em>{\text{opt}}\cdot x+b</em>{\text{opt}}$ 将训练数据集完全正确分开；且存在 $\gamma &gt; 0$，对所有的 $i=1,2,\cdots,N$<br>$$<br>y<em>i(\hat w</em>{\text{opt}} \cdot \hat x_i) = y<em>i(w</em>{\text{opt}}\cdot x<em>i + b</em>{\text{opt}}) \geq \gamma<br>$$</li>
<li>令 $R = \max\limits_{1\leq i\leq N}\Vert \hat x_i\Vert$，则感知机算法在训练数据集上的误分类次数 $k$ 满足不等式<br>$$<br>k \leq \left( \frac{R}{\gamma} \right)^2<br>$$</li>
</ol>
<p>上面定理直白来说就是，如果是一个线性可分的数据集，我们可以在有限 $k$ 次更新，得到一个将数据集完美分割好的超平面（感知机模型）。</p>
<p><strong>Proof:</strong></p>
<ol>
<li>由于训练数据集是线性可分的，存在超平面可将数据集完全正确分开，取此超平面为 $\hat w<em>{\text{opt}}\cdot \hat x = w</em>{\text{opt}}\cdot x + b<em>{\text{opt}} = 0$，使得 $\Vert \hat w</em>{\text{opt}} \Vert=1$.（ $w<em>{\text{opt}}$ 与 $b</em>{\text{opt}}$ 同时缩小或扩大不改变超平面）</li>
</ol>
<p>由于对有限的 $i=1,2,\cdots,N$，均有<br>$$<br>y<em>i(\hat w</em>{\text{opt}}\cdot \hat x_i) = y<em>i(w</em>{\text{opt}} \cdot x<em>i + b</em>{\text{opt}}) &gt; 0<br>$$<br>上式为已经全部分对情形，所以所有训练集上的数据点均满足上式。<br>对于有限个数据点，存在<br>$$<br>\gamma = \min\limits_i { y<em>i(w</em>{\text{opt}} \cdot x<em>i + b</em>{\text{opt}}) }<br>$$<br>使得<br>$$<br>y<em>i(\hat w</em>{\text{opt}} \cdot \hat x_i) = y<em>i(w</em>{\text{opt}}\cdot x<em>i + b</em>{\text{opt}}) \geq \gamma<br>$$</p>
<ol>
<li>感知机算法从 $\hat w<em>0 = 0$ 开始，如果实例被误分类，则更新权重。令 $\hat w</em>{k-1}$ 为第 $k$ 个误分类实例之前的扩充权重向量，即<br>$$<br>\hat w<em>{k-1} = (w</em>{k-1}^T, b_{k-1})^T<br>$$<br>则第 $k$ 个误分类实例的条件是<br>$$<br>y<em>i(\hat w</em>{k-1}\cdot \hat x_i) = y<em>i(w</em>{k-1}\cdot x<em>i+b</em>{k-1}) \leq 0<br>$$<br>若 $(x_i, y<em>i)$ 是被 $\hat w</em>{k-1} = (w<em>{k-1}^T, b</em>{k-1})^T$ 误分类的数据，则 $w$ 和 $b$ 的更新是<br>$$<br>\begin{split}<br>&amp;w<em>k \leftarrow w</em>{k-1} + \eta\,y_ix_i \<br>&amp;b<em>k \leftarrow b</em>{k-1} + \eta\,y_i<br>\end{split}<br>$$<br>即<br>$$<br>\hat w<em>k = \hat w</em>{k-1} + \eta\, y_i\hat x_i<br>$$<br>下面我们证明两个小结论<br>$$<br>\begin{split}<br>&amp;\hat w<em>k \cdot \hat w</em>{\text{opt}} \geq k\eta\,\gamma \<br>&amp;\Vert \hat w_k\Vert^2 \leq k\eta^2R^2<br>\end{split}<br>$$</li>
</ol>
<p>(1) 证明：<br>$$<br>\begin{split}<br>\hat w<em>k\cdot \hat w</em>{\text{opt}} &amp;= \hat w<em>{k-1}\cdot \hat w</em>{\text{opt}} + \eta\, y<em>i\hat w</em>{\text{opt}}\cdot \hat x<em>i \<br>&amp;\geq \hat w</em>{k-1}\cdot \hat w_{\text{opt}} + \eta\,\gamma<br>\end{split}<br>$$<br>由此递推得到不等式<br>$$<br>\hat w<em>k\cdot \hat w</em>{\text{opt}} \geq \hat w<em>{k-1}\cdot \hat w</em>{\text{opt}} + \eta\,\gamma \geq \hat w<em>{k-2}\cdot \hat w</em>{\text{opt}} + 2\eta\,\gamma \geq \cdots \geq k\eta\gamma<br>$$</p>
<p>(2) 证明：<br>$$<br>\begin{split}<br>\Vert \hat w<em>k\Vert^2 &amp;= \Vert \hat w</em>{k-1}\Vert^2 + 2\eta\,y<em>i\hat w</em>{k-1}\cdot \hat x_i + \eta^2\Vert \hat x<em>i\Vert^2 \<br>&amp;\leq \Vert \hat w</em>{k-1}\Vert^2 + \eta^2\Vert \hat x<em>i\Vert^2 \<br>&amp;\leq \Vert \hat w</em>{k-1}\Vert^2 + \eta^2 R^2 \<br>&amp;\leq \Vert \hat w_{k-2}\Vert^2 + 2\eta^2 R^2 \leq \cdots \<br>&amp;\leq k\,\eta^2 R^2<br>\end{split}<br>$$</p>
<p>结合以上，有<br>$$<br>\begin{split}<br>&amp;k\eta\,\gamma \leq \hat w<em>k\cdot \hat w</em>{\text{opt}} \leq \Vert \hat w<em>k\Vert\,\Vert w</em>{\text{opt}}\Vert \leq \sqrt{k}\eta R \<br>&amp;k^2 \gamma^2 \leq k R^2<br>\end{split}<br>$$</p>
<p><strong>Remark:</strong></p>
<ol>
<li>误分类的次数 $k$ 是上界的 $\implies$ 当训练数据集线性可分的时候，感知机学习算法原始迭代是收敛的。</li>
<li>感知机学习算法存在许多解，依赖于初值选择，也依赖于迭代过程中误分类点的选择顺序。</li>
<li>为了得到唯一的超平面，需要对分离超平面增加约束条件，SVM想法由此而来。</li>
<li>对于线性不可分的数据，SVM算法不能收敛，会产生震荡现象。<br>解决方法：<ul>
<li>规定最大迭代次数</li>
<li>每次更新算法的参数当且仅当在该参数下误分割的样本数量减少了</li>
</ul>
</li>
</ol>
<h3 id="3-3-感知机学习算法的对偶形式"><a href="#3-3-感知机学习算法的对偶形式" class="headerlink" title="3.3 感知机学习算法的对偶形式"></a>3.3 感知机学习算法的对偶形式</h3><p>对偶形式就是将参数 $w,b$ 表示为实例 $x_i$ 和 $y_i$ 的线性组合的形式，通过求解其系数而求得 $w$ 和 $b$。我们假设 $w_0,b_0$ 均为0，对误分类点 $(x_i, y_i)$ 通过：<br>$$<br>\begin{split}<br>&amp;w \leftarrow w + \eta\,y_ix_i \<br>&amp;b \leftarrow b + \eta\,y_i<br>\end{split}<br>$$<br>逐步修改 $w, b$，设修改了 $n$ 次，则 $w, b$ 关于 $(x_i, y_i)$ 的增量分别是 $\alpha_i y_i x_i$ 和 $\alpha_i y_i$，这里 $\alpha_i = n<em>i\,\eta$。这样，从学习过程可以得到最终学习到的 $w,b$ 分别为<br>$$<br>\begin{split}<br>&amp;w = \sum</em>{i=1}^N \alpha_i y_i x<em>i \<br>&amp;b = \sum</em>{i=1}^N \alpha_i y_i<br>\end{split}<br>$$<br>这里，$\alpha_i\geq 0$, $i=1,2,\cdots,N$，==当 $\eta=1$ 时，表示 $i$ 个实例点由于误分而进行更新的次数==。<u>实例点更新次数越多，意味着它距离分离超平面越近，也就越难正确分类（越容易分错，超平面移动不多就容易将这些点分错）换句话说，这样的实例对学习结果影响最大</u>（在SVM中，这些点代表着支持向量）</p>
<p>那么我们怎么得到它的对偶形式呢？将 $x_i, y<em>i$ 表达的 $w$ 代入原来感知机模型中，得到下面对偶感知机模型：<br>$$<br>f(x) = \text{sign}\left( \sum</em>{j=1}^N \alpha_j\, y_jx_j\cdot x+b \right)<br>$$<br>根据上面模型方程，我们可以看出与原始感知机模型不同的就是 $w$ 的形式有所改变，那么到底为什么有对偶形式出现呢？后面会讲原因！</p>
<h4 id="Algorithm-2"><a href="#Algorithm-2" class="headerlink" title="Algorithm 2:"></a><strong>Algorithm 2:</strong></h4><p>Input: 线性可分的数据集 $T = { (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N) }$，其中 $x_i\in\mathbb{R}^n, y<em>i\in{ -1, +1 }, i=1,2,\cdot,N$；学习率 $\eta, 0&lt;\eta\leq 1$<br>Output: $a, b$；感知机模型 $f(x) = \text{sign}\left( \sum</em>{j=1}^N \alpha_j\, y_jx_j\cdot x+b \right)$，其中 $\alpha = (\alpha_1, \alpha_2, \cdots, \alpha_N)^T$</p>
<ol>
<li>$\alpha \leftarrow 0, b \leftarrow 0$</li>
<li>在训练集中选取数据 $(x_i, y_i)$</li>
<li>如果 $y<em>i\left( \sum</em>{j=1}^N \alpha_j\,y_jx_j\cdot x_i + b \right) \leq 0$<br>$$<br>\begin{split}<br>&amp;\alpha_i \leftarrow \alpha_i + \eta \<br>&amp;b \leftarrow b + \eta\, y_i<br>\end{split}<br>$$</li>
<li>转到2. 直到没有误分类数据</li>
</ol>
<p><strong>Remark:</strong><br><u>这里的更新其实等价于 $\alpha_i = n<em>i\,\eta$，从 $\alpha</em>{i+1} = \alpha<em>{i} + \eta$ 可以推出 $n</em>{i+1}\eta = n<em>i\eta + \eta$ 进而 $n</em>{i+1} = n_i + 1$，表示如果该数据点分错了，那么更新次数加一，$b$ 的更新方式和原始感知机模型更新方式一样。</u></p>
<p>现在假设样本点 $(x_i, y_i)$ 在更新过程中被使用了 $n<em>i$ 次。因此，从原始形式的学习过程中可以得到，最后学习到的 $w,b$分别可以表示为<br>$$<br>\begin{split}<br>&amp;w = \sum</em>{i=1}^N n_i\,\eta y_ix<em>i \<br>&amp;b = \sum</em>{i=1}^N n_i\,\eta y_i<br>\end{split}<br>$$</p>
<p>考虑 $n_i$ 的含义：如果 $n_i$ 的值越大，那么意味着这个样本点经常被误分，也就说明该点离超平面很近。这种点其实就很可能是支持向量。</p>
<p>现将上式代入感知机模型中，可得：<br>$$<br>f(x) = \text{sign}(w\cdot x + b) = \text{sign}\left( \sum_{j=1}^N n_j\eta\, y_jx<em>j\cdot x + \sum</em>{j=1}^N n_j\eta\, y_j \right)<br>$$</p>
<p>此时，学习的目标不再是 $w,b$，而是 $n_i, i=1,2,\cdots, N.$ 相应的，训练过程变为：</p>
<ol>
<li>初始时刻 $n_i = 0 \quad \forall i = 1,2,\cdots, N$</li>
<li>在训练集中选取数据 $(x_i, y_i)$</li>
<li>如果 $y<em>i\left( \sum\limits</em>{j=1}^N n_j\eta\, y_jx_j\cdot x<em>i + \sum\limits</em>{j=1}^N n_j\eta\, y_j \right) \leq 0$，更新 $n_i \leftarrow n_i + 1$</li>
<li>转到 2. 直至没有误分类数据。</li>
</ol>
<p><em>Question:</em><br>为什么引入对偶形式?</p>
<p><em>Answer:</em><br>根据查阅到的资料，我能接受的观点如下：</p>
<ol>
<li>从对偶形式学习算法过程可以看出，样本点的特征向量以內积的形式存在于感知机对偶形式的训练算法中，凡是涉及到矩阵，向量內积的运算量就非常大（现实中特征维度很高），这里我们如果事先计算好所有的內积，存储于Gram矩阵中，以后碰到更新的点，直接从Gram矩阵中查找即可，相当于我就初始化运算一遍Gram矩阵，以后都是查询，大大加快了计算速度。</li>
</ol>
<blockquote>
<p>不妨假设特征空间是 $\mathbb{R}^n$，$n$很大，一共有 $N$ 个训练数据，$N$ 相对 $n$ 很小。我们考虑原始形式的感知机学习算法，每一轮迭代中我们至少都要判断某个输入实例是不是误判点，即对于 $x_i,y_i$，是否有 $y_i(w x_i + b) \leq 0$。这里的运算量主要集中在求输入实例 $x_i$ 和权值向量 $w$ 的內积上，$\Theta(n)$ 的时间复杂度，由于特征空间维度很高，所以很慢。</p>
<p>而在对偶形式的感知机学习算法中，对于输入实例 $(x_i, y_i)$ 是否误判的条件转变为了 $y<em>i\left( \sum</em>{j=1}^N \alpha_j\,y_jx_j\cdot x_i + b \right) \leq 0$。这里所有输入实例都仅仅以內积的形式出现，所以我们可以预先计算输入实例两两之间的內积，得到所谓的Gram矩阵 $G = [x_i\cdot x<em>j]</em>{N\times N}$。这样一来每次做误判检测的时候我们直接在Gram矩阵里查表就能拿到內积 $x_i\cdot x_j$，所以这个误判检测的时间复杂度是 $\Theta(N)$。</p>
<p>也就是说，对偶形式的感知机，把每轮迭代的时间复杂度的数据规模从特征空间维度 $n$ 转移到了训练集大小 $N$ 上，那么对于维度非常高的空间，自然可以提升性能了。</p>
</blockquote>
<ol>
<li>跟SVM的对偶形式其实有相似之处，后面讲到SVM的时候再说明。</li>
</ol>
<p><strong>Example:</strong><br>用感知机学习算法的对偶形式求解感知机模型，数据同上。</p>
<p>按照对偶算法框架，有：</p>
<ol>
<li>取 $\alpha_i = 0, i=1,2,3, b=0, \eta=1$</li>
<li>计算Gram矩阵<br>$$<br>G = \begin{bmatrix}<br>18 &amp; 21 &amp; 6 \<br>21 &amp; 25 &amp; 7 \<br>6 &amp; 7 &amp; 2<br>\end{bmatrix}<br>$$</li>
<li>误分条件<br>$$<br>y<em>i \left( \sum</em>{j=1}^N \alpha_j y_j x_j \cdot x_i + b \right) \leq 0<br>$$<br>参数更新<br>$$<br>\alpha_i \leftarrow \alpha_i+1 \quad b \leftarrow b + y_i<br>$$</li>
<li>迭代。</li>
<li>​<br>$$<br>\begin{split}<br>&amp;w = 2x_1 + 0x_2 - 5x_3 = (1,1)^T \<br>&amp;b = -3<br>\end{split}<br>$$<br>分离超平面<br>$$<br>x^{(1)} + x^{(2)} - 3 = 0<br>$$<br>感知机模型<br>$$<br>f(x) = \text{sign}(x^{(1)} + x^{(2)} - 3)<br>$$</li>
</ol>
<p>迭代过程中的参数变化如下表</p>
<table>
<thead>
<tr>
<th>$k$</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td></td>
<td>$x_1$</td>
<td>$x_3$</td>
<td>$x_3$</td>
<td>$x_3$</td>
<td>$x_1$</td>
<td>$x_3$</td>
<td>$x_3$</td>
</tr>
<tr>
<td>$\alpha_1$</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>2</td>
<td>2</td>
<td>2</td>
</tr>
<tr>
<td>$\alpha_2$</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr>
<td>$\alpha_3$</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>2</td>
<td>3</td>
<td>3</td>
<td>4</td>
<td>5</td>
</tr>
<tr>
<td>$b$</td>
<td>0</td>
<td>1</td>
<td>0</td>
<td>-1</td>
<td>-2</td>
<td>-1</td>
<td>-2</td>
<td>-3</td>
</tr>
</tbody>
</table>
<p>对比原始形式的例子，可以发现结果一致且迭代步骤也是对应的。</p>
<p>用python实现该对偶问题的计算如下：</p>
<ul>
<li><p>初始化数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">train_set = np.array([[[<span class="number">3</span>, <span class="number">3</span>], <span class="number">1</span>], [[<span class="number">4</span>, <span class="number">3</span>], <span class="number">1</span>], [[<span class="number">1</span>, <span class="number">1</span>], <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">a = np.zeros(len(train_set), np.float)</span><br><span class="line">b = <span class="number">0.0</span></span><br><span class="line">max_iter = <span class="number">1000</span></span><br><span class="line">Gram = <span class="keyword">None</span></span><br><span class="line">y = np.array(train_set[:, <span class="number">1</span>])</span><br><span class="line">x = np.empty((len(train_set), <span class="number">2</span>), np.float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">    x[i] = train_set[i][<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>主函数逻辑</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    Gram = cal_gram()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> misclassified():</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>cal_gram()</code>函数计算Gram矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_gram</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    calculate the Gram matrix</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    g = np.empty((len(train_set), len(train_set)), np.int)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">            g[i][j] = np.dot(train_set[i][<span class="number">0</span>], train_set[j][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> g</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>misclassified()</code>函数判断是否分类正确</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">misclassified</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a, b, x, y</span><br><span class="line">    is_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">        <span class="keyword">if</span> product(i) &lt;= <span class="number">0</span>:</span><br><span class="line">            is_correct = <span class="keyword">True</span></span><br><span class="line">            update(i)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_correct:</span><br><span class="line">        w = np.dot(a * y, x)</span><br><span class="line">        print(<span class="string">"\nResult within max iterations:"</span>)</span><br><span class="line">        print(<span class="string">"w: "</span>, w, <span class="string">" b: "</span>, b)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>product()</code>函数计算误分条件左端值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a, b, x, y</span><br><span class="line">    </span><br><span class="line">    sum = np.dot(a * y, Gram[i])</span><br><span class="line">    sum = (sum + b) * y[i]</span><br><span class="line">    <span class="keyword">return</span> sum</span><br></pre></td></tr></table></figure>
</li>
<li><p>编写<code>update()</code>函数用于更新参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    update parameter using stochastic gradient descent</span></span><br><span class="line"><span class="string">    :param i:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> a, b</span><br><span class="line">    a[i] += <span class="number">1</span></span><br><span class="line">    b += y[i]</span><br><span class="line">    print(a, b)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>程序运行结果如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[ 1.  0.  0.] 1.0</span><br><span class="line">[ 1.  0.  1.] 0.0</span><br><span class="line">[ 1.  0.  2.] -1.0</span><br><span class="line">[ 1.  0.  3.] -2.0</span><br><span class="line">[ 2.  0.  3.] -1.0</span><br><span class="line">[ 2.  0.  4.] -2.0</span><br><span class="line">[ 2.  0.  5.] -3.0</span><br><span class="line"></span><br><span class="line">Result within max iterations:</span><br><span class="line">w:  [1.0 1.0]  b:  -3.0</span><br></pre></td></tr></table></figure></p>
<p>完整的程序代码如下<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Dual problem solution</span></span><br><span class="line">train_set = np.array([[[<span class="number">3</span>, <span class="number">3</span>], <span class="number">1</span>], [[<span class="number">4</span>, <span class="number">3</span>], <span class="number">1</span>], [[<span class="number">1</span>, <span class="number">1</span>], <span class="number">-1</span>]])</span><br><span class="line"></span><br><span class="line">a = np.zeros(len(train_set), np.float)</span><br><span class="line">b = <span class="number">0.0</span></span><br><span class="line">max_iter = <span class="number">1000</span></span><br><span class="line">Gram = <span class="keyword">None</span></span><br><span class="line">y = np.array(train_set[:, <span class="number">1</span>])</span><br><span class="line">x = np.empty((len(train_set), <span class="number">2</span>), np.float)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">    x[i] = train_set[i][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cal_gram</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    calculate the Gram matrix</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    g = np.empty((len(train_set), len(train_set)), np.int)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">            g[i][j] = np.dot(train_set[i][<span class="number">0</span>], train_set[j][<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    update parameter using stochastic gradient descent</span></span><br><span class="line"><span class="string">    :param i:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">global</span> a, b</span><br><span class="line">    a[i] += <span class="number">1</span></span><br><span class="line">    b += y[i]</span><br><span class="line">    print(a, b)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">product</span><span class="params">(i)</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a, b, x, y</span><br><span class="line">    </span><br><span class="line">    sum = np.dot(a * y, Gram[i])</span><br><span class="line">    sum = (sum + b) * y[i]</span><br><span class="line">    <span class="keyword">return</span> sum</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">misclassified</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">global</span> a, b, x, y</span><br><span class="line">    is_correct = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(train_set)):</span><br><span class="line">        <span class="keyword">if</span> product(i) &lt;= <span class="number">0</span>:</span><br><span class="line">            is_correct = <span class="keyword">True</span></span><br><span class="line">            update(i)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> is_correct:</span><br><span class="line">        w = np.dot(a * y, x)</span><br><span class="line">        print(<span class="string">"\nResult within max iterations:"</span>)</span><br><span class="line">        print(<span class="string">"w: "</span>, w, <span class="string">" b: "</span>, b)</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">True</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    Gram = cal_gram()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(max_iter):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> misclassified():</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Remark:</strong><br>与原始形式一样，感知机学习算法的对偶形式迭代也是收敛的，且存在多个解。</p>
<p><strong>Summary:</strong></p>
<ol>
<li>感知机是二类，线性分类模型。要求给定数据线性可分</li>
<li>感知机算法本质：求一个超平面，使得预定的损失函数最小化</li>
<li>感知机的模型, 策略, 算法分别为<ul>
<li>model: 超平面（对二维空间就是直线），线性模型。若样本维数为 $n$，假设空间 $\mathbb{R}^{n+1}$</li>
<li>strategy: 极小化损失函数</li>
<li>algorithm: gradient descent</li>
</ul>
</li>
<li>超平面方程：$w\cdot x+b=0$，$w,x$ 是与样本 $x$ 相同维数的向量</li>
<li>损失函数：$-\sum \frac{1}{\Vert w\Vert} y_i(w\cdot x_i+b)$，只考虑所有被错误分类的点。感知机算法即优化这样一个函数</li>
<li>感知机的形式：$\text{sign}(w\cdot x+b)$，$\text{sign}(x)$ 是符号函数</li>
</ol>
<h2 id="4-感知机学习算法应用实例"><a href="#4-感知机学习算法应用实例" class="headerlink" title="4. 感知机学习算法应用实例"></a>4. 感知机学习算法应用实例</h2><h3 id="4-1-鸢尾花数据集分类"><a href="#4-1-鸢尾花数据集分类" class="headerlink" title="4.1. 鸢尾花数据集分类"></a>4.1. 鸢尾花数据集分类</h3><h4 id="python源码实现感知机"><a href="#python源码实现感知机" class="headerlink" title="python源码实现感知机"></a>python源码实现感知机</h4><ul>
<li>加载需要用到的包</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd  <span class="comment"># pandas用数据提取与展示</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np  <span class="comment"># numpy用于数组操作</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris  <span class="comment"># 从sklearn中导入数据集</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt  <span class="comment"># matplotlib用于制图可视化</span></span><br></pre></td></tr></table></figure>
<ul>
<li>数据提取</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iris = load_iris()  <span class="comment"># 加载iris数据集并命名</span></span><br><span class="line">df = pd.DataFrame(iris.data, columns=iris.feature_names)  <span class="comment"># 将数据集放入pandas数据框</span></span><br><span class="line">df[<span class="string">'label'</span>] = iris.target  <span class="comment"># 将iris的目标值记为名为label的列</span></span><br><span class="line">df  <span class="comment"># 可以看到数据框了，类似 150*50 的矩阵</span></span><br></pre></td></tr></table></figure>
<ul>
<li>简单数据统计</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.columns = [<span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>, <span class="string">'petal length'</span>, <span class="string">'petal width'</span>, <span class="string">'label'</span>]  <span class="comment"># 对DataFrame的列重命名</span></span><br><span class="line">df.label.value_counts()  <span class="comment"># 对label的值进行简要的统计</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可视化数据点</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">plt.scatter(df[:<span class="number">50</span>][<span class="string">'sepal length'</span>], df[:<span class="number">50</span>][<span class="string">'sepal width'</span>], label=<span class="string">'0'</span>)  <span class="comment"># 选取前50个值，绘制散点图，横纵坐标分别为length, width</span></span><br><span class="line">plt.scatter(df[<span class="number">50</span>:<span class="number">100</span>][<span class="string">'sepal length'</span>], df[<span class="number">50</span>:<span class="number">100</span>][<span class="string">'sepal width'</span>], label=<span class="string">'1'</span>)  <span class="comment"># 选取后50个值，绘制散点图，同上</span></span><br><span class="line">plt.xlabel(<span class="string">'sepal length'</span>)  <span class="comment"># 给x轴加label</span></span><br><span class="line">plt.ylabel(<span class="string">'sepal width'</span>)  <span class="comment"># 给y轴加label</span></span><br><span class="line">plt.legend()  <span class="comment"># 加上图例</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>另一种选取数据方式</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">data = np.array(df.iloc[:<span class="number">100</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>]])  <span class="comment"># 选取第1,2和最后一列组成新的数据框</span></span><br><span class="line">X, y = data[:, :<span class="number">-1</span>], data[:, <span class="number">-1</span>]  <span class="comment"># X为除最后一列的所有，y为最后一列</span></span><br><span class="line">y = np.array([<span class="number">1</span> <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y])  <span class="comment"># 将y中所有0变为-1</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>定义Perceptron类</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = np.ones(len(data[<span class="number">0</span>])<span class="number">-1</span>, dtype=np.float32)</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.l_rate = <span class="number">0.1</span></span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sign</span><span class="params">(self, x, w, b)</span>:</span></span><br><span class="line">        y = np.dot(x, w) + b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        is_wrong = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_wrong:</span><br><span class="line">            wrong_count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> range(len(X_train)):</span><br><span class="line">                X = X_train[d]</span><br><span class="line">                y = y_train[d]</span><br><span class="line">                <span class="keyword">if</span> y * self.sign(X, self.w, self.b) &lt;= <span class="number">0</span>:</span><br><span class="line">                    self.w = self.w + self.l_rate * np.dot(y, X)</span><br><span class="line">                    self.b = self.b + self.l_rate * y</span><br><span class="line">                    wrong_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> wrong_count == <span class="number">0</span>:</span><br><span class="line">                is_wrong = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">'Perceptron Model'</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>
<ul>
<li>用感知机模型训练数据</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perceptron = Perceptron()  <span class="comment"># 创建Perceptron的一个实例</span></span><br><span class="line">perceptron.fit(X, y)  <span class="comment"># 传入数据开始训练</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可视化训练得到的w和b</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x_points = np.linspace(<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">y_ = (perceptron.w[<span class="number">0</span>] * x_points + perceptron.b) / perceptron.w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x_points, y_)</span><br><span class="line">plt.plot(data[:<span class="number">50</span>, <span class="number">0</span>], data[:<span class="number">50</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'blue'</span>, label=<span class="string">'0'</span>)</span><br><span class="line">plt.plot(data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'orange'</span>, label=<span class="string">'1'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sepal width'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/iris_1.png" width="300"><br></div>

<p>从上图可以看到，训练结果近似符合预期，但还不够好，下面利用高度优化的<code>sklearn</code>包来进行感知机方法的训练。</p>
<p>完整的程序代码如下：<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">df = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = iris.target</span><br><span class="line"></span><br><span class="line">df.columns = [<span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>,</span><br><span class="line">              <span class="string">'petal length'</span>, <span class="string">'petal width'</span>, <span class="string">'label'</span>]</span><br><span class="line">data = np.array(df.iloc[:<span class="number">100</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>]])</span><br><span class="line">X, y = data[:, :<span class="number">-1</span>], data[:, <span class="number">-1</span>]</span><br><span class="line">y = np.array([<span class="number">1</span> <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Perceptron</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.w = np.ones(len(data[<span class="number">0</span>]) - <span class="number">1</span>, dtype=np.float32)</span><br><span class="line">        self.b = <span class="number">0</span></span><br><span class="line">        self.l_rate = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sign</span><span class="params">(self, x, w, b)</span>:</span></span><br><span class="line">        y = np.dot(x, w) + b</span><br><span class="line">        <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fit</span><span class="params">(self, X_train, y_train)</span>:</span></span><br><span class="line">        is_wrong = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">while</span> <span class="keyword">not</span> is_wrong:</span><br><span class="line">            wrong_count = <span class="number">0</span></span><br><span class="line">            <span class="keyword">for</span> d <span class="keyword">in</span> range(len(X_train)):</span><br><span class="line">                X = X_train[d]</span><br><span class="line">                y = y_train[d]</span><br><span class="line">                <span class="keyword">if</span> y * self.sign(X, self.w, self.b) &lt;= <span class="number">0</span>:</span><br><span class="line">                    self.w = self.w + self.l_rate * np.dot(y, X)</span><br><span class="line">                    self.b = self.b + self.l_rate * y</span><br><span class="line">                    wrong_count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> wrong_count == <span class="number">0</span>:</span><br><span class="line">                is_wrong = <span class="keyword">True</span></span><br><span class="line">            <span class="keyword">return</span> <span class="string">'Perceptron Model'</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">score</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">perceptron = Perceptron()</span><br><span class="line">perceptron.fit(X, y)</span><br><span class="line"></span><br><span class="line">x_points = np.linspace(<span class="number">4</span>, <span class="number">7</span>, <span class="number">10</span>)</span><br><span class="line">y_ = (perceptron.w[<span class="number">0</span>] * x_points + perceptron.b) / perceptron.w[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x_points, y_)</span><br><span class="line">plt.plot(data[:<span class="number">50</span>, <span class="number">0</span>], data[:<span class="number">50</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'blue'</span>, label=<span class="string">'0'</span>)</span><br><span class="line">plt.plot(data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'orange'</span>, label=<span class="string">'1'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sepal width'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p>
<h4 id="利用sklearn实现感知机方法"><a href="#利用sklearn实现感知机方法" class="headerlink" title="利用sklearn实现感知机方法"></a>利用sklearn实现感知机方法</h4><ul>
<li>导包</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br></pre></td></tr></table></figure>
<ul>
<li>定义一个分类器并开始训练</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">clf = Perceptron(fit_intercept=<span class="keyword">False</span>, n_iter=<span class="number">1000</span>, shuffle=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(X, y)</span><br></pre></td></tr></table></figure>
<ul>
<li>查看训练结果</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(clf.coef_)  <span class="comment"># 训练得到的系数w</span></span><br><span class="line">print(clf.intercept_)  <span class="comment"># 训练得到的截距b</span></span><br></pre></td></tr></table></figure>
<ul>
<li>可视化呈现</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x_points = np.arange(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">y_ = -(clf.coef_[<span class="number">0</span>][<span class="number">0</span>] * x_points + clf.intercept_) / clf.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x_points, y_)</span><br><span class="line"></span><br><span class="line">plt.plot(data[:<span class="number">50</span>, <span class="number">0</span>], data[:<span class="number">50</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'blue'</span>, label=<span class="string">'0'</span>)</span><br><span class="line">plt.plot(data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'orange'</span>, label=<span class="string">'1'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sepal width'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/iris_2.png" width="300"><br></div>

<p>完整代码如下：<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"></span><br><span class="line">iris = load_iris()</span><br><span class="line">df = pd.DataFrame(iris.data, columns=iris.feature_names)</span><br><span class="line">df[<span class="string">'label'</span>] = iris.target</span><br><span class="line"></span><br><span class="line">df.columns = [<span class="string">'sepal length'</span>, <span class="string">'sepal width'</span>,</span><br><span class="line">              <span class="string">'petal length'</span>, <span class="string">'petal width'</span>, <span class="string">'label'</span>]</span><br><span class="line">data = np.array(df.iloc[:<span class="number">100</span>, [<span class="number">0</span>, <span class="number">1</span>, <span class="number">-1</span>]])</span><br><span class="line">X, y = data[:, :<span class="number">-1</span>], data[:, <span class="number">-1</span>]</span><br><span class="line">y = np.array([<span class="number">1</span> <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">else</span> <span class="number">-1</span> <span class="keyword">for</span> i <span class="keyword">in</span> y])</span><br><span class="line"></span><br><span class="line">clf = Perceptron(fit_intercept=<span class="keyword">False</span>, n_iter=<span class="number">1000</span>, shuffle=<span class="keyword">False</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line"><span class="comment"># print(clf.coef_)</span></span><br><span class="line"><span class="comment"># print(clf.intercept_)</span></span><br><span class="line"></span><br><span class="line">x_points = np.arange(<span class="number">4</span>, <span class="number">8</span>)</span><br><span class="line">y_ = -(clf.coef_[<span class="number">0</span>][<span class="number">0</span>] * x_points + clf.intercept_) / clf.coef_[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(x_points, y_)</span><br><span class="line"></span><br><span class="line">plt.plot(data[:<span class="number">50</span>, <span class="number">0</span>], data[:<span class="number">50</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'blue'</span>, label=<span class="string">'0'</span>)</span><br><span class="line">plt.plot(data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], data[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="string">'bo'</span>, color=<span class="string">'orange'</span>, label=<span class="string">'1'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'sepal length'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sepal width'</span>)</span><br><span class="line">plt.legend()</span><br></pre></td></tr></table></figure></p>
<h3 id="4-2-从感知机到人工神经网络ANN"><a href="#4-2-从感知机到人工神经网络ANN" class="headerlink" title="4.2. 从感知机到人工神经网络ANN"></a>4.2. 从感知机到人工神经网络ANN</h3><p>人造神经网络的概念主要受模拟大脑中神经元的目标的启发。但现在它变成了一个工程和计算机科学的主题，并且在大规模视觉识别和机器学习任务中显示出有前途的结果。<br>我们不需要过多考虑生物学的细节例如生物神经元与感知器的比较。因为了解生物神经元对于理解感知器的工作原理并不是绝对必要的。<br>只是为了理解，下面的图片是一个生物神经元，描述一个神经元做什么的流程。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/10/neuron.png" width="400"><br></div>

<p>神经科学界的共识是，人脑包含1000亿个神经元，每个神经元有1万个突触，数量巨大，组合方式复杂，联系广泛。也就是说，突触传递机制十分复杂。</p>
<p>现在已经发现和阐明的突触传递机制有：突触后兴奋、突触后抑制、突触前抑制、突触前兴奋，以及远程抑制等。</p>
<p>ANN人工神经网络是一种模仿生物神经网络结构和功能的数学模型，它使用大量的人工神经元连接来进行计算，该网络由大量的“神经元”相互连接构成，每个“神经元”代表一种特定的输出函数。又称激励函数。每两个“神经元”间的连接代表一个通过该连接信号的加权值，称之为权重，这相当于人工神经网络的记忆。网络的输出则根据网络的连接规则来确定，输出因权重值和激励函数的不同而不同。人工神经网络可以理解为对自然界某种算法或者函数的逼近。</p>
<p>下面是一种模拟上述生物神经元的感知器模型。图片来自于Andrej Karpathy在斯坦福课程<a href="http://cs231n.github.io/neural-networks-1/" target="_blank" rel="noopener">CS231n</a>上的讲座。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/10/neuron_model.jpeg" width="400"><br></div>

<p>我们可以看到每个神经元或者感知器执行一个带有输入和它的权重的点积，用它们加上偏差，然后应用非线性 $f(x)$，在这种情况下是sigmoid。这个非线性函数也被称为激活函数。</p>
<p>就上图而言，总输入是 $x = x_1 + x_2 + \ldots + x_N$，其中 $N$ 是输入的总数。类别预测取决于特定样本的激活是否导致 $f(z)$ 的输出大于预定阈值。该阈值包含在公式 $z = w_1 x_1 + w_2 x_2 + b$ 中，正如我们在上图中看到的那样，阈值 $b$ 又被称为偏差。为了使它更普遍，有时用 $w_0$ 代替，乘以一个对应的 $x_0$ 得到最终的样子 $z = w_0 x_0 + w_1 x_1 + w_2 x_2$。从图形上看，它看起来像下图，我们在数据中有两个特征 $x_1$ 和 $x_2$。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/10/linearly-separable.png" width="250"><br></div>

<p>对于更高维的数据，分界线将是一个超平面。</p>
<p>Rosenblatt感知机是由没过计算机科学家F.Rosenblatt于1957年提出的。F.Rosenblatt经过证明得出结论，如果两类模式是线性可分的，则算法一定收敛。Rosenblatt感知器特别适用于简单的模式分类问题，也可用于基于模式分类的学习控制。</p>
<h3 id="4-3-单层感知机求解AND-OR问题"><a href="#4-3-单层感知机求解AND-OR问题" class="headerlink" title="4.3. 单层感知机求解AND/OR问题"></a>4.3. 单层感知机求解AND/OR问题</h3><h4 id="原始解法"><a href="#原始解法" class="headerlink" title="原始解法"></a>原始解法</h4><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUM_FEATURES = <span class="number">2</span></span><br><span class="line">NUM_ITER = <span class="number">2000</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]], np.float32)  <span class="comment"># 4x2, input</span></span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], np.float32)  <span class="comment"># 4, correct output, AND operation</span></span><br><span class="line"><span class="comment"># y = np.array([0, 1, 1, 1], np.float32) # OR operation</span></span><br><span class="line"></span><br><span class="line">W = np.zeros(NUM_FEATURES, np.float32)  <span class="comment"># 2x1, weight</span></span><br><span class="line">b = np.zeros(<span class="number">1</span>, np.float32)  <span class="comment"># 1x1</span></span><br><span class="line"></span><br><span class="line">N, d = np.shape(x)  <span class="comment"># number of samples and number of features</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># process each sample separately</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(NUM_ITER):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(N):</span><br><span class="line">        yHat_j = x[j, :].dot(W) + b  <span class="comment"># 1x2, 2x1</span></span><br><span class="line">        yHat_j = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-yHat_j))</span><br><span class="line"></span><br><span class="line">        err = y[j] - yHat_j  <span class="comment"># error term</span></span><br><span class="line">        deltaW = err * x[j, :]</span><br><span class="line">        deltaB = err</span><br><span class="line">        W = W + learning_rate * deltaW  <span class="comment"># if err = y - yHat, then W = W + lRate * deltW</span></span><br><span class="line">        b = b + learning_rate * deltaB</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now plot the fitted line. We need only two points to plot the line</span></span><br><span class="line">plot_x = np.array([np.min(x[:, <span class="number">0</span>] - <span class="number">0.2</span>), np.max(x[:, <span class="number">1</span>] + <span class="number">0.2</span>)])</span><br><span class="line"><span class="comment"># comes from, w0*x + w1*y + b = 0 then y = (-1/w1) (w0*x + b)</span></span><br><span class="line">plot_y = - <span class="number">1</span> / W[<span class="number">1</span>] * (W[<span class="number">0</span>] * plot_x + b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'W:'</span> + str(W))</span><br><span class="line">print(<span class="string">'b:'</span> + str(b))</span><br><span class="line">print(<span class="string">'plot_y: '</span> + str(plot_y))</span><br><span class="line"></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">100</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.plot(plot_x, plot_y, color=<span class="string">'k'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.2</span>, <span class="number">1.2</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.2</span>, <span class="number">1.25</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>结果如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W:[ 2.6949501   2.69091272]</span><br><span class="line">b:[-4.2682209]</span><br><span class="line">plot_y: [ 1.78646111  0.38436049]</span><br></pre></td></tr></table></figure></p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/AND_OR_1.png" width="300"><br></div>

<p>除了这种每次迭代中逐一处理每个样本的方法，我们可以对公式进行矢量化以减少额外的for循环，使程序运行得更快。下面给出等价的更加简单的实现。矢量化编程在机器学习中非常有用。</p>
<figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUM_FEATURES = <span class="number">2</span></span><br><span class="line">NUM_ITER = <span class="number">2000</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]], np.float32)  <span class="comment"># 4x2, input</span></span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], np.float32)  <span class="comment"># 4, correct output, AND operation</span></span><br><span class="line"><span class="comment"># y = np.array([0, 1, 1, 1], np.float32) # OR operation</span></span><br><span class="line"></span><br><span class="line">W = np.zeros(NUM_FEATURES, np.float32)  <span class="comment"># 2x1, weight</span></span><br><span class="line">b = np.zeros(<span class="number">1</span>, np.float32)  <span class="comment"># 1x1</span></span><br><span class="line"></span><br><span class="line">N, d = np.shape(x)  <span class="comment"># number of samples and number of features</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(NUM_ITER):</span><br><span class="line">    yHat = x.dot(W) + b</span><br><span class="line">    yHat = <span class="number">1.0</span> / (<span class="number">1.0</span> + np.exp(-yHat))</span><br><span class="line"></span><br><span class="line">    err = y - yHat</span><br><span class="line"></span><br><span class="line">    deltaW = np.transpose(x).dot(err)  <span class="comment"># have to 2x1</span></span><br><span class="line">    deltaB = np.sum(err)  <span class="comment"># have to 1x1. collect error from all the 4 samples</span></span><br><span class="line">    W = W + learning_rate * deltaW  <span class="comment"># if err = y - yHat, then W = W + lRate * deltW</span></span><br><span class="line">    b = b + learning_rate * deltaB</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now plot the fitted line. We need only two points to plot the line</span></span><br><span class="line">plot_x = np.array([np.min(x[:, <span class="number">0</span>] - <span class="number">0.2</span>), np.max(x[:, <span class="number">1</span>] + <span class="number">0.2</span>)])</span><br><span class="line"><span class="comment"># comes from, w0*x + w1*y + b = 0 then y = (-1/w1) (w0*x + b)</span></span><br><span class="line">plot_y = - <span class="number">1</span> / W[<span class="number">1</span>] * (W[<span class="number">0</span>] * plot_x + b)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'W:'</span> + str(W))</span><br><span class="line">print(<span class="string">'b:'</span> + str(b))</span><br><span class="line">print(<span class="string">'plot_y: '</span> + str(plot_y))</span><br><span class="line"></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">100</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.plot(plot_x, plot_y, color=<span class="string">'k'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.2</span>, <span class="number">1.2</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.2</span>, <span class="number">1.25</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>要特别注意<code>deltaW</code>和<code>deltaB</code>，我们直接将输入<code>x</code>转进行转置，然后乘以误差项。这用到了一个简单的线性代数技巧。最简单的方法就是考虑矩阵大小，输入<code>x</code>是 $4\times 2$ 的，<code>err</code>是 $4\times 1$ 的，那么<code>weights</code>必须是 $2\times 1$ 的。为了得到 $2\times 1$ 矩阵，这里我们直接取了 $x^T\cdot (y-\hat y)$。对于<code>deltaB</code>我们直接求和，得到所有样本点的误差。程序运行求得结果与之前非常接近</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W:[ 2.68957829  2.68957829]</span><br><span class="line">b:[-4.26430988]</span><br><span class="line">plot_y: [ 1.78549385  0.38549384]</span><br></pre></td></tr></table></figure>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/AND_OR_2.png" width="300"><br></div>

<h4 id="利用TensorFlow求解single-perceptron"><a href="#利用TensorFlow求解single-perceptron" class="headerlink" title="利用TensorFlow求解single-perceptron"></a>利用TensorFlow求解single-perceptron</h4><p>现在，让我们使用tensorflow实现相同的感知器算法，看看利用TensorFlow来解决问题的基本流程。在面对海量数据的时候，TensorFlow将会是一个强大的工具。</p>
<p>在TensorFlow中，除了直接定义<code>x</code>和<code>y</code>以外，我们还需要在一个<code>session</code>下定义<code>x</code>和<code>y</code>对应的<code>placeholder</code>，而<code>Variable</code>被申明为TensorFlow中的变量，我们需要在程序运行过程中训练它。TensorFlow函数<code>Variable()</code>有一个名为<code>trainable=True</code>的默认值参数。默认为True时会将其放入计算图进行计算。这里起初并没有更改原先的<code>W</code>和<code>B</code>，而是利用了临时变量<code>W_</code>和<code>B_</code>，将每次迭代更新后得到的参数存储，然后再将它们的值<code>assign</code>到原始的变量<code>W</code>和<code>B</code>中。</p>
<p>完整的程序代码如下：<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUM_FEATURES = <span class="number">2</span></span><br><span class="line">NUM_ITER = <span class="number">2000</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>]], np.float32)  <span class="comment"># 4x2, input</span></span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>], np.float32)  <span class="comment"># 4, correct output, AND operation</span></span><br><span class="line"><span class="comment"># y = np.array([0, 1, 1, 1], np.float32) # OR operation</span></span><br><span class="line">y = np.reshape(y, [<span class="number">4</span>, <span class="number">1</span>])  <span class="comment"># convert to 4x1</span></span><br><span class="line"></span><br><span class="line">X = tf.placeholder(tf.float32, shape=[<span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line">Y = tf.placeholder(tf.float32, shape=[<span class="number">4</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">W = tf.Variable(tf.zeros([NUM_FEATURES, <span class="number">1</span>]), tf.float32)</span><br><span class="line">B = tf.Variable(tf.zeros([<span class="number">1</span>, <span class="number">1</span>]), tf.float32)</span><br><span class="line"></span><br><span class="line">yHat = tf.sigmoid(tf.add(tf.matmul(X, W), B))  <span class="comment"># 4x1</span></span><br><span class="line">err = Y - yHat</span><br><span class="line">deltaW = tf.matmul(tf.transpose(X), err)  <span class="comment"># have to be 2x1</span></span><br><span class="line">deltaB = tf.reduce_sum(err, <span class="number">0</span>)  <span class="comment"># 4, have to 1x1. sum all the biases? yes</span></span><br><span class="line">W_ = W + learning_rate * deltaW</span><br><span class="line">B_ = B + learning_rate * deltaB</span><br><span class="line"></span><br><span class="line"><span class="comment"># to update the values of weights and biases.</span></span><br><span class="line">step = tf.group(W.assign(W_), B.assign(B_))</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(NUM_ITER):</span><br><span class="line">    sess.run([step], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line"></span><br><span class="line">W = np.squeeze(sess.run(W))</span><br><span class="line">b = np.squeeze(sess.run(B))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Now plot the fitted line. We need only two points to plot the line</span></span><br><span class="line">plot_x = np.array([np.min(x[:, <span class="number">0</span>] - <span class="number">0.2</span>), np.max(x[:, <span class="number">1</span>] + <span class="number">0.2</span>)])</span><br><span class="line">plot_y = - <span class="number">1</span> / W[<span class="number">1</span>] * (W[<span class="number">0</span>] * plot_x + b)</span><br><span class="line">plot_y = np.reshape(plot_y, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">plot_y = np.squeeze(plot_y)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'W: '</span> + str(W))</span><br><span class="line">print(<span class="string">'b: '</span> + str(b))</span><br><span class="line">print(<span class="string">'plot_y: '</span> + str(plot_y))</span><br><span class="line"></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">100</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.plot(plot_x, plot_y, color=<span class="string">'k'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.xlim([<span class="number">-0.2</span>, <span class="number">1.2</span>])</span><br><span class="line">plt.ylim([<span class="number">-0.2</span>, <span class="number">1.25</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>程序运行结果如下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">W: [ 2.68957829  2.68957829]</span><br><span class="line">b: -4.264309883117676</span><br><span class="line">plot_y: [ 1.78549385  0.38549384]</span><br></pre></td></tr></table></figure></p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/AND_OR_3.png" width="300"><br></div>

<p>从上图可以看到，单层感知机模型对于逻辑与/或问题，给出了很好的结果。</p>
<h4 id="多层感知机求解XOR问题"><a href="#多层感知机求解XOR问题" class="headerlink" title="多层感知机求解XOR问题"></a>多层感知机求解XOR问题</h4><p>下面我们从一个例子开始介绍多层感知机模型(multi-layer perceptron)。同样，多层感知机是一种监督学习方法。在面临XOR问题时，我们发现single-layer perceptron不起作用了！为了学习XOR问题的特征，我们需要一个至少两层的神经网络，因为XOR问题不能由简单一条直线进行区分。下面我们简单实现一个两层的神经网络来学习XOR模型。对于MNIST手写数字集，我们也可以用类似的多层感知机来进行分类。</p>
<p>对于如下逻辑门</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/11/xor_0.png?w=666&h=240" width="200"><br></div>

<p>将输入<code>A</code>和<code>B</code>记为<code>x</code>，输出记为<code>y</code>，异或问题在<code>x</code>中元素一致时取<code>0</code>，否则取<code>1</code>。</p>
<p>对于为什么需要两层神经网络有一个直观的理解，如下图</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/11/xor_11.png" width="300"><br></div>

<p>黄点为0，红点为1，那么我们需要两条线来将<code>0</code>和<code>1</code>划分。我们已经看到，神经元/感知器只给我们一条线，将输入空间分成两类。所以我们至少需要在我们第一个隐藏层中的两个神经元来学习异或。我们需要将这两个神经元并排放置在一个层中，而不是放在两个不同的层中，以便他们同时看到输入并学习如何分离输入空间。</p>
<p>下面实现以下最简单的两层神经网络来学习异或门。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/xor_2.png" width="300"><br></div>

<p><strong>Remark:</strong><br>值得注意的是，一个MLP可以包含任意数量的层和任意数量的神经元。这些通常被称作是超参数(hyperparameters)。如何选择超参数是一个算法设计问题，由实际问题的大小和难度来决定。例如，XOR是一个具有两个特征并且输入大小仅为4的玩具问题。因此，第一个隐藏层中具有2个单位的双层MLP就能够学习XOR函数。一个比较大的问题，例如对mnist数据集中的数字进行分类，将需要在每个layer中使用更多的神经元，后面有机会会讲到。</p>
<p><strong>MLP模型确立</strong><br>我们在单层感知器中已经看到输出是通过将输入x乘以权重w，加上偏差b并最终采用一个非线性sigmoid函数f来激活计算的。<br>如上图，我们有一个输入层，一个隐藏层和一个输出层，w中的上标表示图层编号。<br>多层感知器有时被称为普通神经网络(vanilla neural networks)，特别是当它们只有一个隐藏层时。</p>
<p>有趣的是，人工神经网络(ANN)的基本结构与单层感知器相似。我们将在所有层中进行相同的计算。<br>最初我们将我们的输入传到第一个隐藏层，然后第一层的输出被输入到第二层。也就是说，每次我们将当前层的输出视为下一层的输入并执行类似的计算。<br>隐藏层输出可以通过如下表示</p>
<p>$$<br>h = g(W^{(1)} x + b^{(1)})<br>$$<br>其中，$x$ 为inputs, $g$ 为一个non-linear的激活函数，$W^{(1)}$ 是第一层上的weights，$b$ 是bias。将隐藏层看做输入层，我们可以得到MLP最终层的输出</p>
<p>$$<br>\hat y = f(W^{(2)} h + b^{(2)})<br>$$</p>
<p>结合以上两个式子（这个过程称为feed-forward）可得</p>
<p>$$<br>\hat y = f(W^{(2)} g(W^{(1)} x + b^{(1)}) + b^{(2)})<br>$$</p>
<p>实际上，无论MLP中有多少图层，数学表达式都是单个方程，其中考虑了从输入开始的所有隐藏层。<br>简言之，每个多层感知器基于训练数据集学都了某个函数，并且能够将相似的输入序列映射到适当的输出。</p>
<p><strong>Remark:</strong></p>
<ol>
<li>在任何层中，由于权重 $W$ 用于将输入传递到输出，所以它被定义为前后神经元层数的矩阵。例如，在我们的MLP中，$W^{(1)}$是2×2，$W^{(2)}$是2×1。</li>
<li>非线性 $g$ 在神经网络中起着重要作用。最常见的非线性类型包括sigmoid，tanh，relu等，以及它们各自的优点和局限性。nonlinear也称为激活函数。</li>
<li>feed-forward图解</li>
</ol>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/feed_forward.png" height="500"><br></div>

<p><strong>python实现多层感知机求解异或问题</strong><br>不要忘记导入必要的包含和其他全局变量，如学习速率，迭代次数等等，重要的是要注意最小尺寸的MLP（2个输入，2个隐藏的神经元，1个输出神经元），正如我们在这里实现的，学习XOR可能会很棘手。<br>可能需要调整learning rate。太大的值（如0.1）是不常用，因为会使网络振荡overshoot。非常低的值（如0.0001）将导致网络学习非常缓慢，可能需要迭代数十万次。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], np.float32)</span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], np.float32)</span><br></pre></td></tr></table></figure>
<p>下面实现两层感知器模型的辅助函数<code>multi_layer_perceptron_xor</code>。我们使用S型非线性作为激活函数。也可以用其他非线性的函数例如tanh。</p>
<p><strong>Remark:</strong><br>观察显示tanh比sigmoid具有更高的收敛概率，但这里还是sigmoid。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_layer_perceptron_xor</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line"></span><br><span class="line">    hidden_layer1 = tf.add(tf.matmul(x, weights[<span class="string">'w_h1'</span>]), biases[<span class="string">'b_h1'</span>])</span><br><span class="line">    hidden_layer1 = tf.nn.sigmoid(hidden_layer1)</span><br><span class="line"></span><br><span class="line">    out_layer = tf.add(tf.matmul(hidden_layer1, weights[<span class="string">'w_out'</span>]), biases[<span class="string">'b_out'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br></pre></td></tr></table></figure>
<p>权重值的初始化对学习XOR也很重要。我们从一个随机正态分布中选择权重，其均值为0.0，标准差为1.0，并将weights的数据类型设成python字典。</p>
<p><strong>Remark:</strong></p>
<ol>
<li>sigmoid和tanh都有一个输出饱和的巨大区域。这些区域的梯度非常小，这在神经网络中不是好的性质。一般我们希望我们的权重在原点周围足够小，以使激活函数在其线性区域中运行，这部分梯度最大。</li>
<li>有时候随机初始化可能会遇到一种参数组合，该情形下很容易陷入局部最小值，并且网络不会学到任何东西。所以你在调整参数以减少迭代次数的时候，可能会遇到网络出现两条随机线而不是XOR的解决方案的情况。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w_h1'</span> : tf.Variable(tf.random_normal([num_input, num_hidden1])), <span class="comment"># w1, from input layer to hidden layer 1</span></span><br><span class="line">    <span class="string">'w_out'</span>: tf.Variable(tf.random_normal([num_hidden1, num_output])) <span class="comment"># w2, from hidden layer 1 to output layer</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b_h1'</span> : tf.Variable(tf.zeros([num_hidden1])),</span><br><span class="line">    <span class="string">'b_out'</span>: tf.Variable(tf.zeros([num_output]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>创建模型:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = multi_layer_perceptron_xor(X, weights, biases)</span><br></pre></td></tr></table></figure></p>
<p>现在我们需要训练模型。为此，我们定义了一个损失函数和优化器optimizer。由于这是一个二元分类问题，因此使用sigmoid交叉熵损失而不是softmax。<br>MLP利用一种称为反向传播（因为我们已有标签y）的监督学习方法进行训练。<u>反向传播是任何人工神经网络设计的核心。简而言之，它是通过计算成本函数(cost function)的梯度来调整神经元权重的一种方法。它从输出层开始，并将错误传播回第一层，以便神经元以减少前一次迭代的误差的方式来调整权重。通过这种方式，整个网络最终以一组权重值来较好的解释训练集。</u><br>反向传播是一个很大的话题，涉及大量的数学，值得在这方面发表一篇完整的文章。<br>TensorFlow在这里为我们完成所有这些后台计算，包括梯度，反向传播和权重更新的工作。在这里，我们只需要关注MLP如何工作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_func = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=model, labels=Y))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_func)</span><br></pre></td></tr></table></figure>
<p>用最初设置的迭代次数进行迭代。确保你用足够的迭代训练模型。XOR通常需要大量迭代才能收敛，推荐至少100,000次迭代。你可以使用不同的迭代次数来观察网络的行为。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(num_iter):</span><br><span class="line">    tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">    <span class="keyword">if</span> k % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#print('output: ', sess.run(model, feed_dict=&#123;X:x&#125;))</span></span><br><span class="line">        print(<span class="string">'loss= '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(tmp_cost))</span><br></pre></td></tr></table></figure>
<p>通过output层，可以绘制出如下的拟合线</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/11/xor_3.png" width="300"><br></div>

<p>完整代码如下<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers cmd</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">num_features = <span class="number">2</span></span><br><span class="line">num_iter = <span class="number">10000</span></span><br><span class="line">display_step = int(num_iter / <span class="number">10</span>)</span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">num_input = <span class="number">2</span>          <span class="comment"># units in the input layer 28x28 images</span></span><br><span class="line">num_hidden1 = <span class="number">2</span>        <span class="comment"># units in the first hidden layer</span></span><br><span class="line">num_output = <span class="number">1</span>         <span class="comment"># units in the output, only one output 0 or 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#%% mlp function</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_layer_perceptron_xor</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line"></span><br><span class="line">    hidden_layer1 = tf.add(tf.matmul(x, weights[<span class="string">'w_h1'</span>]), biases[<span class="string">'b_h1'</span>])</span><br><span class="line">    hidden_layer1 = tf.nn.sigmoid(hidden_layer1)</span><br><span class="line"></span><br><span class="line">    out_layer = tf.add(tf.matmul(hidden_layer1, weights[<span class="string">'w_out'</span>]), biases[<span class="string">'b_out'</span>])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"><span class="comment">#%%</span></span><br><span class="line">x = np.array([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]], np.float32)  <span class="comment"># 4x2, input</span></span><br><span class="line">y = np.array([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], np.float32)                      <span class="comment"># 4, correct output, AND operation</span></span><br><span class="line">y = np.reshape(y, [<span class="number">4</span>,<span class="number">1</span>])                                    <span class="comment"># convert to 4x1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># trainum_inputg data and labels</span></span><br><span class="line">X = tf.placeholder(<span class="string">'float'</span>, [<span class="keyword">None</span>, num_input])     <span class="comment"># training data</span></span><br><span class="line">Y = tf.placeholder(<span class="string">'float'</span>, [<span class="keyword">None</span>, num_output])    <span class="comment"># labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># weights and biases</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w_h1'</span> : tf.Variable(tf.random_normal([num_input, num_hidden1])), <span class="comment"># w1, from input layer to hidden layer 1</span></span><br><span class="line">    <span class="string">'w_out'</span>: tf.Variable(tf.random_normal([num_hidden1, num_output])) <span class="comment"># w2, from hidden layer 1 to output layer</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b_h1'</span> : tf.Variable(tf.zeros([num_hidden1])),</span><br><span class="line">    <span class="string">'b_out'</span>: tf.Variable(tf.zeros([num_output]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">model = multi_layer_perceptron_xor(X, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="string">'''</span></span><br><span class="line"><span class="string">- cost function and optimization</span></span><br><span class="line"><span class="string">- sigmoid cross entropy -- single output</span></span><br><span class="line"><span class="string">- softmax cross entropy -- multiple output, normalized</span></span><br><span class="line"><span class="string">'''</span></span><br><span class="line">loss_func = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=model, labels=Y))</span><br><span class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss_func)</span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> range(num_iter):</span><br><span class="line">    tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict=&#123;X: x, Y: y&#125;)</span><br><span class="line">    <span class="keyword">if</span> k % display_step == <span class="number">0</span>:</span><br><span class="line">        <span class="comment">#print('output: ', sess.run(model, feed_dict=&#123;X:x&#125;))</span></span><br><span class="line">        print(<span class="string">'loss= '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(tmp_cost))</span><br><span class="line"></span><br><span class="line"><span class="comment"># separates the input space</span></span><br><span class="line">W = np.squeeze(sess.run(weights[<span class="string">'w_h1'</span>]))   <span class="comment"># 2x2</span></span><br><span class="line">b = np.squeeze(sess.run(biases[<span class="string">'b_h1'</span>]))    <span class="comment"># 2,</span></span><br><span class="line"></span><br><span class="line">sess.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#%%</span></span><br><span class="line"><span class="comment"># Now plot the fitted line. We need only two points to plot the line</span></span><br><span class="line">plot_x = np.array([np.min(x[:, <span class="number">0</span>] - <span class="number">0.2</span>), np.max(x[:, <span class="number">1</span>]+<span class="number">0.2</span>)])</span><br><span class="line">plot_y =  <span class="number">-1</span> / W[<span class="number">1</span>, <span class="number">0</span>] * (W[<span class="number">0</span>, <span class="number">0</span>] * plot_x + b[<span class="number">0</span>])</span><br><span class="line">plot_y = np.reshape(plot_y, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">plot_y = np.squeeze(plot_y)</span><br><span class="line"></span><br><span class="line">plot_y2 = <span class="number">-1</span> / W[<span class="number">1</span>, <span class="number">1</span>] * (W[<span class="number">0</span>, <span class="number">1</span>] * plot_x + b[<span class="number">1</span>])</span><br><span class="line">plot_y2 = np.reshape(plot_y2, [<span class="number">2</span>, <span class="number">-1</span>])</span><br><span class="line">plot_y2 = np.squeeze(plot_y2)</span><br><span class="line"></span><br><span class="line">plt.scatter(x[:, <span class="number">0</span>], x[:, <span class="number">1</span>], c=y, s=<span class="number">100</span>, cmap=<span class="string">'viridis'</span>)</span><br><span class="line">plt.plot(plot_x, plot_y, color=<span class="string">'k'</span>, linewidth=<span class="number">2</span>)    <span class="comment"># line 1</span></span><br><span class="line">plt.plot(plot_x, plot_y2, color=<span class="string">'k'</span>, linewidth=<span class="number">2</span>)   <span class="comment"># line 2</span></span><br><span class="line">plt.xlim([<span class="number">-0.2</span>, <span class="number">1.2</span>]); plt.ylim([<span class="number">-0.2</span>, <span class="number">1.25</span>]);</span><br><span class="line"><span class="comment">#plt.text(0.425, 1.05, 'XOR', fontsize=14)</span></span><br><span class="line">plt.xticks([<span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>]); plt.yticks([<span class="number">0.0</span>, <span class="number">0.5</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p><strong>背后细节:</strong><br>我们已经看到了一个学习XOR的MLP模型。问题是，这个网络是怎样训练出两条可以很好地区分两种输入类型的线？让我们解码一个mlp的内幕。<br>遵照我们上面建立的XOR网络，我们可以解剖每个神经元的输出，如下面的逻辑表。这里有趣的一点是可以看到第一个隐藏层单元h1和h2学到了什么。</p>
<table>
<thead>
<tr>
<th>x1</th>
<th>x2</th>
<th>h1</th>
<th>h2</th>
<th>y</th>
</tr>
</thead>
<tbody>
<tr>
<td>0</td>
<td>0</td>
<td>0</td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>0</td>
<td>1</td>
<td>1</td>
<td>1</td>
</tr>
<tr>
<td>1</td>
<td>1</td>
<td>1</td>
<td>0</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>在下面的图中，暗区显示1，而亮区意味着0。第一隐藏单元h1学习权重，以便如下图（红色超平面）所示，它将输入序列中x1和x2均为0的输入与其余的分开。<br>h2以同样的方式分隔输入，如图蓝色超平面。<br>这种方式网络提出了以下两种中间解决方案。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/xor_41.png" width="300"><br></div>

<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/xor_52.png" width="300"><br></div>

<p>你会发现每个h1和h2实际上都是作为单层感知器工作的，其中每个单元实际上都学习了一个单独的线（或超平面）。</p>
<p><strong>Remark:</strong><br>你也可以把h1当作OR门，把h2当作NAND门。</p>
<p>再观察上面的逻辑表，这两个中间输出将作为输出神经元y的输入。输出单元在其两个输入（h1和h2）为1时响应1，否则保持沉默（或在生物学术语中不反应）。输出y组合了两个超平面，最后我们找到下面的图，它和我们已经看到的xor输出相同。其中，暗区指示1，亮区指示0。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/xor_6.png" width="300"><br></div>


<h3 id="4-4-利用多层感知机训练MNIST数据集"><a href="#4-4-利用多层感知机训练MNIST数据集" class="headerlink" title="4.4. 利用多层感知机训练MNIST数据集"></a>4.4. 利用多层感知机训练MNIST数据集</h3><p>上面，我们了一个简单的两层MLP来求解XOR问题。为了看到mlp的实际潜力，我们应该设计一个具有两层以上的适度更大的MLP并且看看它在真实世界数据集上是如何工作的。</p>
<p>我们选择mnist作为数据集来实现我们的mlp。尽管mnist被认为是机器学习社区中非常简单的数据集之一，但我们仍然选择这个数据集，因为这将使我们清楚地了解多层感知器的工作原理，并有助于使我们做好使用其他大数据集的准备。后面，我们还可以用mnist数据集来做其他一些很酷的事情，比如使用一些降维技术将每个图像视为2D空间中的一个点。让我们来谈谈关于mnist数据集的一些问题，因为我们在机器学习研究中经常遇到这些数据。</p>
<p>MNIST是手写数字的数据库，由Yann Lecun，Corinna Cortes和Christopher.J.C. Burges创建。有大约60000次训练和10000个测试图像/示例。它是一个名为NIST的更大集合的子集。这些数字已经进行了尺寸标准化并以固定尺寸的图像为中心。图像是 $28\times 28$ 尺寸大小的。作为一个测试平台，我们可以在MNIST数据集上尝试各种学习算法和模式识别方法，同时保持很低的预处理和格式化方面的开销，这使得MNIST成为机器学习中使用最广泛的数据集之一。我们不必担心自己下载数据集，只需要TensorFlow里的一条命令<code>tensorflow.examples.tutorials.mnist</code>就可以获取。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'/path/to/MNIST/'</span>, one_hot=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>首先，我们来看几个样例：</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/mlp2_2.png" width="300"><br></div>

<p>图像是 $28\times 28$ 大小的并且随机的。训练数据由 $55,000$ 个样本组成，因此大小是 $55000\times 784$。训练的labels是 $55000\times 10$ 大小的（10是类别数量0到9）而不是 $55000\times 1$。这是因为它们采用one-hot向量的格式，其中只有相应的位置标签为1，其余为零。例如，如果训练样本是2，则相应的单热矢量将是[0,0,1,0,0,0,0,0,0,0]。</p>
<p>下图是MNIST样本在2d空间中的快照。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/mlp2_mnist_vis.png" width="300"><br></div>

<p>我们看到10个集群，分别对应10位数字。一些集群相互交织在一起，这是因为有些数字看起来很相似。例如，数字“4”和数字“9”在手写中有时看起来相似。所以很可能这两个数字在像素空间中彼此靠近。<br>可视化是在将维数从784减少到2（使用t-SNE）之后创建的。后面有机会会讲一些维数降低技术。</p>
<p><strong>MLP模型:</strong><br>以下是我们要实施的mlp。我们有784个输入像素，用<code>x</code>表示，128个<code>h1</code>神经元，256个<code>h2</code>神经元和10个<code>y</code>或输出神经元。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/mlp2_model.png" width="300"><br></div>

<p>我们在单层感知器中已经看到，输出 $\hat y$ 是通过将输入 $x$ 乘以权重 $W$，加上偏差 $b$ 并最终采用非线性sigmoid函数 $f$ 来计算的。这里我们有一个输入层，两个隐藏层和一个输出层，如上图所示，$W$ 中的上标依旧表示网络层数编号。</p>
<p>ANN的基本构件与单层感知器相似。我们将在所有层中进行相同的计算。最初我们有我们的输入到第一个隐藏层，然后第一个隐藏层的输出被输入到第二个隐藏层。<br>每次我们将当前输出视为下一层的输入并执行类似的计算。隐藏层输出公式如下</p>
<p>$$<br>h1 = g(W^{(1)} x + b^{(1)})<br>$$</p>
<p>现在隐藏层输出<code>h1</code>将作为第二隐藏层<code>h2</code>的输入，并且<code>h2</code>层的输出作为输入到输出层<code>y</code>的输入。</p>
<p>$$<br>h2 = g(W^{(2)} h1 + b^{(2)})<br>$$</p>
<p>结合上式，有</p>
<p>$$<br>h2 = g(W^{(2)} g(W^{(1)} x + b(1)) + b^{(2)})<br>$$</p>
<p>得最后的输出</p>
<p>$$<br>\hat y = f(W^{(3)} h2 + b^{(3)})<br>$$</p>
<p>这里没有将它们全部结合起来，用x，h1和h2表示y。因为看起来会很杂乱。实际上，我们在前面已经说过，无论MLP中有多少图层，数学表达式都可以是输出的单个方程，其中包括了从输入开始的所有隐藏层。简言之，每个多层感知器基于训练数据集学习单个函数 $f(\cdot)$，并且能够将相似的输入序列映射到适当的输出。</p>
<p><strong>Remark:</strong><br>依然作为附注，在任何层中，由于权重<code>W</code>用于将输入传递到输出，所以它通常被定义为前后神经元层数之间的矩阵。例如，在我们的mlp中，<code>W1</code>是<code>784x128</code>，<code>W2</code>是<code>128x256</code>，<code>W3</code>是<code>256x10</code>。</p>
<p><strong>多层神经网络:</strong><br>MLP能做的不只是学习一个简单的异或门。基于以上，我们可以非常轻松地插入新layer来完成更苛刻的分类。让我们再添加一层到目前为止实现的层。</p>
<p>在XOR中，我们在输出层只有一个神经元，输出0或1。MNIST是一个数字的数据库，所以现在我们有10个输出神经元表示从0到9。</p>
<p>假设我们在第一个隐藏层中有128个神经元，第二个中有256个神经元。<br>现在我们有10个输出神经元，输入是784维，它们是图像的每个像素。<br>我们在每次迭代中一次处理100个图像，这就是为什么我们将批量大小设置为100（可以是小于图像总数的任何其他数字）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">num_input = <span class="number">784</span>  </span><br><span class="line">num_hidden1 = <span class="number">128</span></span><br><span class="line">num_hidden2 = <span class="number">256</span></span><br><span class="line">num_output = <span class="number">10</span></span><br></pre></td></tr></table></figure>
<p>以下我们定义神经网络的辅助函数。使用我们在上一篇文章中看到的相同的数学表达式，我们定义了1层，2层等等。你可以看到在另一个层增加一层很容易。这里我们在输出层之前添加了隐藏层2。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_layer_perceptron_mnist</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    MLP model with more than 2 hidden layers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    hidden_layer1 = tf.add(tf.matmul(x, weights[<span class="string">'w_h1'</span>]), biases[<span class="string">'b_h1'</span>])</span><br><span class="line">    hidden_layer1 = tf.nn.relu(hidden_layer1)   <span class="comment"># apply ReLU non-linearity</span></span><br><span class="line">    hidden_layer2 = tf.add(tf.matmul(hidden_layer1, weights[<span class="string">'w_h2'</span>]), biases[<span class="string">'b_h2'</span>])</span><br><span class="line">    hidden_layer2 = tf.nn.relu(hidden_layer2)</span><br><span class="line"></span><br><span class="line">    out_layer = tf.add(tf.matmul(hidden_layer2, weights[<span class="string">'w_out'</span>]), biases[<span class="string">'b_out'</span>])  <span class="comment"># NO non-linearity in the output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br></pre></td></tr></table></figure>
<p>对于我们的三层MLP，我们定义每个层的权重和偏差为python字典。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w_h1'</span> : tf.Variable(tf.random_normal([num_input, num_hidden1])),       <span class="comment"># w1, from input layer to hidden layer 1</span></span><br><span class="line">    <span class="string">'w_h2'</span> : tf.Variable(tf.random_normal([num_hidden1, num_hidden2])),     <span class="comment"># w2, from hidden layer 1 to hidden layer 2</span></span><br><span class="line">    <span class="string">'w_out'</span>: tf.Variable(tf.random_normal([num_hidden2, num_output]))       <span class="comment"># w3, from hidden layer 2 to output layer</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b_h1'</span> : tf.Variable(tf.random_normal([num_hidden1])),                  <span class="comment"># b1, to hidden layer 1 units</span></span><br><span class="line">    <span class="string">'b_h2'</span> : tf.Variable(tf.random_normal([num_hidden2])),</span><br><span class="line">    <span class="string">'b_out'</span>: tf.Variable(tf.random_normal([num_output]))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意，在这里我们使用了一个不同的损失函数，<code>softmax</code>带logits的交叉熵，我们用来学习异或操作的东西是sigmoid。原因是这个数据集有两个以上的类。S形交叉熵损失仅用于二元分类问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_func)</span><br></pre></td></tr></table></figure>
<p>您可能会注意到，不是使用梯度下降作为优化器，而是使用adam作为优化器。adam代表自适应动量估计。它是随机梯度下降算法的替代方法之一，它自己为每个参数自适应地更新学习速率。adam是类似adagrad，adadelta，rmsprop等其他优化算法的升级，它计算每个参数的自适应学习速率。除了存储过去平方梯度的指数衰减平均值之外，它还保持与动量相似的过去梯度的指数衰减平均值。</p>
<p>现在训练模型进行一些迭代。一起处理55000个图像，这被称为批处理，在计算的上下文中是低效的。另一方面，逐一处理每个图像，这被称为随机处理，也有不利的一面。由于每个梯度都是基于单个训练样例进行计算的，因此误差比噪声梯度下降更大。在这两个极端情况下，我们提出了一种小批量技术，我们在这里称之为batch_size，在此处处理一大块图像。小批量学习可以理解为将批量梯度下降应用于训练数据的较小子集，例如一次100个样本。与批量梯度下降相比，优势在于通过小批量更快地达到收敛，因为更频繁的权重更新。没有关于将多少图像用作批量大小的定义规则。</p>
<p>正如我们在上一篇文章中提到的，MLP利用监督式学习（因为我们提供了标签）技术，称为反向传播训练。<u>反向传播是任何人工神经网络设计的核心。简而言之，它是通过计算成本函数的梯度来调整神经元权重的一种方法。它从输出层开始，并将错误传播回第一层，以便神经元以减少前一次迭代错误的方式调整权重。</u>通过这种方式，整个网络以一组权重值来确定训练集（希望测试集也是如此）。tensorflow强在幕后处理所有这些梯度计算，反向传播和权重更新。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(num_iter):</span><br><span class="line">    avg_cost = <span class="number">0.0</span></span><br><span class="line">    num_batch = int(mnist.train.num_examples / batch_size)   <span class="comment"># total number of batches</span></span><br><span class="line">    <span class="keyword">for</span> nB <span class="keyword">in</span> range(num_batch):</span><br><span class="line">        trainData, trainLabels = mnist.train.next_batch(batch_size=batch_size)</span><br><span class="line">        tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict=&#123;x: trainData, y: trainLabels&#125;)</span><br><span class="line"></span><br><span class="line">        avg_cost = avg_cost + tmp_cost / num_batch</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch: %04d'</span> %(iter+<span class="number">1</span>), <span class="string">'cost= '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(avg_cost), <span class="string">'accuracy: '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(acc))</span><br></pre></td></tr></table></figure>
<p>随着迭代的进行，您将看到损失函数在下降，如下图所示。左图显示损失按照迭代进行下降，并在某个点几乎达到饱和。在右侧，准确度从大约90％开始。最初准确性急剧增加，这基本上是损失函数大幅下降的原因。在某个点上，准确度也会在一个很小的范围内得到修正。</p>
<div align="center"><br><img src="https://nasirml.files.wordpress.com/2017/12/mlp2_loss.png" width="200"><br><img src="https://nasirml.files.wordpress.com/2017/12/mlp2_accuracy.png" width="200"><br></div>

<p>我们看到我们的MLP在找到10位数类的非线性分类方面做得非常好。准确度在95.45％左右。每次运行情况可能会有所不同。你可以改变不同层次的单元/神经元的数量，批量大小等，并查看网络的行为。</p>
<p>完整代码如下:<br><figure class="highlight python"><figcaption><span>&#123;.line-numbers&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># %% data</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read mnist data. If the data is there, it will not download again.</span></span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'/path/to/MNIST/'</span>, one_hot=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># look at some of the images. randomly</span></span><br><span class="line">rand_img = np.array([<span class="number">2500</span>, <span class="number">1001</span>, <span class="number">100</span>, <span class="number">500</span>])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(np.size(rand_img, <span class="number">0</span>)):</span><br><span class="line">    plt.subplot(<span class="number">1</span>, <span class="number">4</span>, i+<span class="number">1</span>)</span><br><span class="line">    plt.axis(<span class="string">'off'</span>)</span><br><span class="line">    plt.imshow(np.reshape(mnist.train.images[rand_img[i]], [<span class="number">28</span>, <span class="number">28</span>]), cmap=<span class="string">'gray'</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Training data size : '</span>, mnist.train.images.shape)</span><br><span class="line">print(<span class="string">'Training label size: '</span>, mnist.train.labels.shape)  <span class="comment"># labels are in one-hot vector</span></span><br><span class="line"><span class="comment"># print(mnist.train.labels[rand_img])</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># %% create the MLP model</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multi_layer_perceptron_mnist</span><span class="params">(x, weights, biases)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    MLP model with more than 2 hidden layers.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    hidden_layer1 = tf.add(tf.matmul(x, weights[<span class="string">'w_h1'</span>]), biases[<span class="string">'b_h1'</span>])</span><br><span class="line">    hidden_layer1 = tf.nn.relu(hidden_layer1)   <span class="comment"># apply ReLU non-linearity</span></span><br><span class="line">    hidden_layer2 = tf.add(tf.matmul(hidden_layer1, weights[<span class="string">'w_h2'</span>]), biases[<span class="string">'b_h2'</span>])</span><br><span class="line">    hidden_layer2 = tf.nn.relu(hidden_layer2)</span><br><span class="line"></span><br><span class="line">    out_layer = tf.add(tf.matmul(hidden_layer2, weights[<span class="string">'w_out'</span>]), biases[<span class="string">'b_out'</span>])  <span class="comment"># NO non-linearity in the output layer</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out_layer</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># %% construct the MLP model</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># hyper-parameters</span></span><br><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line">num_iter = <span class="number">30</span></span><br><span class="line">batch_size = <span class="number">100</span></span><br><span class="line">display_step = <span class="number">10</span>       <span class="comment"># display the avg cost after this number of epochs</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># variables</span></span><br><span class="line">num_input = <span class="number">784</span>         <span class="comment"># units in the input layer 28x28 images</span></span><br><span class="line">num_hidden1 = <span class="number">128</span>       <span class="comment"># units in the first hidden layer</span></span><br><span class="line">num_hidden2 = <span class="number">256</span></span><br><span class="line">num_output = <span class="number">10</span>         <span class="comment"># units in the output layer 0 to 9. OR nClasses</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># trainum_inputg data and labels</span></span><br><span class="line">x = tf.placeholder(<span class="string">'float'</span>, [<span class="keyword">None</span>, num_input])     <span class="comment"># training data</span></span><br><span class="line">y = tf.placeholder(<span class="string">'float'</span>, [<span class="keyword">None</span>, num_output])    <span class="comment"># labels</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># weights and biases</span></span><br><span class="line">weights = &#123;</span><br><span class="line">    <span class="string">'w_h1'</span>: tf.Variable(tf.random_normal([num_input, num_hidden1])),       <span class="comment"># w1, from input layer to hidden layer 1</span></span><br><span class="line">    <span class="string">'w_h2'</span>: tf.Variable(tf.random_normal([num_hidden1, num_hidden2])),     <span class="comment"># w2, from hidden layer 1 to hidden layer 2</span></span><br><span class="line">    <span class="string">'w_out'</span>: tf.Variable(tf.random_normal([num_hidden2, num_output]))       <span class="comment"># w3, from hidden layer 2 to output layer</span></span><br><span class="line">&#125;</span><br><span class="line">biases = &#123;</span><br><span class="line">    <span class="string">'b_h1'</span>: tf.Variable(tf.random_normal([num_hidden1])),                  <span class="comment"># b1, to hidden layer 1 units</span></span><br><span class="line">    <span class="string">'b_h2'</span>: tf.Variable(tf.random_normal([num_hidden2])),</span><br><span class="line">    <span class="string">'b_out'</span>: tf.Variable(tf.random_normal([num_output]))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># construct the model</span></span><br><span class="line">model = multi_layer_perceptron_mnist(x, weights, biases)</span><br><span class="line"></span><br><span class="line"><span class="comment"># cost function and optimization</span></span><br><span class="line">loss_func = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=model, labels=y))</span><br><span class="line">optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss_func)</span><br><span class="line"></span><br><span class="line"><span class="comment"># %% Train and test</span></span><br><span class="line"></span><br><span class="line">sess = tf.Session()</span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">sess.run(init)</span><br><span class="line"></span><br><span class="line">cost_all = np.array([])</span><br><span class="line">acc_all = np.array([])</span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line"><span class="keyword">for</span> iter <span class="keyword">in</span> range(num_iter):</span><br><span class="line">    avg_cost = <span class="number">0.0</span></span><br><span class="line">    num_batch = int(mnist.train.num_examples / batch_size)   <span class="comment"># total number of batches</span></span><br><span class="line">    <span class="keyword">for</span> nB <span class="keyword">in</span> range(num_batch):</span><br><span class="line">        trainData, trainLabels = mnist.train.next_batch(batch_size=batch_size)</span><br><span class="line">        tmp_cost, _ = sess.run([loss_func, optimizer], feed_dict=&#123;x: trainData, y: trainLabels&#125;)</span><br><span class="line"></span><br><span class="line">        avg_cost = avg_cost + tmp_cost / num_batch</span><br><span class="line"></span><br><span class="line">    correct_pred = tf.equal(tf.arg_max(model, <span class="number">1</span>), tf.arg_max(y, <span class="number">1</span>))</span><br><span class="line">    accuracy = tf.reduce_mean(tf.cast(correct_pred, <span class="string">'float'</span>))</span><br><span class="line">    acc = accuracy.eval(session=sess, feed_dict=&#123;x: mnist.test.images, y: mnist.test.labels&#125;)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> iter % display_step == <span class="number">0</span>:</span><br><span class="line">        print(<span class="string">'Epoch: %04d'</span> % (iter+<span class="number">1</span>), <span class="string">'cost= '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(avg_cost), <span class="string">'accuracy: '</span> + <span class="string">"&#123;:.5f&#125;"</span>.format(acc))</span><br><span class="line">    cost_all = np.append(cost_all, avg_cost)</span><br><span class="line">    acc_all = np.append(acc_all, acc)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Optimization done...'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># plot the accuracy and loss</span></span><br><span class="line">x_data = range(num_iter)</span><br><span class="line">plt.plot(x_data, cost_all, color=<span class="string">'r'</span>)</span><br><span class="line">plt.xticks([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line">plt.yticks([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>, <span class="number">40</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">plt.plot(x_data, acc_all)</span><br><span class="line">plt.xticks([<span class="number">0</span>, <span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line">plt.yticks([<span class="number">0.9</span>, <span class="number">0.95</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p>
<p>今天内容就止步于此，相信关于感知机大家还有很多问题，比如就刚才这个问题，对3层MLP的简单扩展将会增加一层，看看它是否会改变网络的结果。从理论上讲，如果将这4层MLP应用于MNIST数据，分类准确性应该更高，至少略高一点。不过事实情况建议大家自己尝试。</p>
<h2 id="5-Exercise"><a href="#5-Exercise" class="headerlink" title="5. Exercise:"></a>5. Exercise:</h2><ol>
<li>用简单神经网络训练逻辑与的运算，尝试不同的学习率</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">1</span>]])</span><br><span class="line">labels = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<ol>
<li><p>将一组 $(x, y)$ 值划分为下面两类函数之一</p>
<ul>
<li>$2x+1=y$ 为第1类</li>
<li>$7x+1=y$ 为第2类</li>
</ul>
<p>训练数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">8</span>], [<span class="number">2</span>, <span class="number">15</span>], [<span class="number">3</span>, <span class="number">7</span>], [<span class="number">4</span>, <span class="number">29</span>]])</span><br><span class="line">labels = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<p>由函数定义可知，$[9, 19]$ 属于第1类，$[9, 64]$ 属于第2类。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">9 19 =&gt; 1</span><br><span class="line">9 64 =&gt; -1</span><br></pre></td></tr></table></figure>
<p>输出训练之后的神经网络权值参数，最后用 $[9, 19]$ 和 $[3, 22]$ 对训练成功的网络进行测试</p>
</li>
<li><p><a href="https://onlookerliu.github.io/2018/02/26/Simple-Perceptron-using-Processing/">可视化感知机训练过程</a></p>
</li>
<li>验证如下数据集上，Rosenblatt感知机算法的局限性</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">inputs = np.array([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">6</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">12</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">9</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">21</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">16</span>], [<span class="number">1</span>, <span class="number">3</span>, <span class="number">15</span>]])</span><br><span class="line">labels = np.array([<span class="number">1</span>, <span class="number">1</span>, <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>])</span><br></pre></td></tr></table></figure>
<p>Hint:</p>
<div align="center"><br><img src="http://p6xgwpfbb.bkt.clouddn.com/perceptron/noLinearSep.png" width="300"><br></div>

<h2 id="6-Reference"><a href="#6-Reference" class="headerlink" title="6. Reference"></a>6. Reference</h2><ol>
<li><p><a href="http://p6xgwpfbb.bkt.clouddn.com/books/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95%20%E6%9D%8E%E8%88%AA.pdf" target="_blank" rel="noopener">李航《统计学习方法》</a></p>
</li>
<li><p><a href="https://web.stanford.edu/~boyd/cvxbook/" target="_blank" rel="noopener">Stephen Boyd, Convex Optimization</a></p>
</li>
<li><p><a href="http://iamtrask.github.io/2015/07/12/basic-python-network/" target="_blank" rel="noopener">A bare bones neural network implementation to describe the inner workings of backpropagation</a></p>
</li>
<li><p><a href="http://natureofcode.com/book/chapter-10-neural-networks/" target="_blank" rel="noopener">The Nature of Code</a></p>
</li>
<li><p><a href="https://appliedgo.net/perceptron/" target="_blank" rel="noopener">Perceptrons - the most basic form of a neural network</a></p>
</li>
<li><p><a href="https://cognitivedemons.wordpress.com/2017/09/02/a-neural-network-in-10-lines-of-cuda-c-code/" target="_blank" rel="noopener">A Neural Network in 10 lines of CUDA C++ Code</a></p>
</li>
<li><p><a href="https://nasirml.wordpress.com/2017/11/19/single-layer-perceptron-in-tensorflow/" target="_blank" rel="noopener">Single-layer Perceptron in TensorFlow</a></p>
</li>
<li><p><a href="https://nasirml.wordpress.com/2017/12/08/multi-layer-perceptron-in-tensorflow-part-1-xor/" target="_blank" rel="noopener">Multilayer Perceptron in TensorFlow Part 1</a></p>
</li>
<li><p><a href="https://nasirml.wordpress.com/2017/12/16/multi-layer-perceptron-in-tensorflow-part-2-mnist/" target="_blank" rel="noopener">Multi-layer Perceptron in TensorFlow Part 2 MNIST</a></p>
</li>
<li><p><a href="https://github.com/serbanc94/mnist-perceptron" target="_blank" rel="noopener">mnist-perceptron</a></p>
</li>
<li><p><a href="https://aimatters.wordpress.com/2016/01/16/solving-xor-with-a-neural-network-in-tensorflow/#comments" target="_blank" rel="noopener">Solving XOR with a Neural Network in TensorFlow</a></p>
</li>
<li><p><a href="https://cseweb.ucsd.edu/~yfreund/papers/LargeMarginsUsingPerceptron.pdf" target="_blank" rel="noopener">Large Margin Classification Using the Perceptron Algorithm</a></p>
</li>
<li><p><a href="https://jizhi.im/blog/post/deep_learning_from_scratch_2" target="_blank" rel="noopener">土法神经网络Part II：感知机</a></p>
</li>
<li><p><a href="https://jizhi.im/blog/post/deep_learning_from_scratch_5" target="_blank" rel="noopener">土法神经网络Part V：多层感知机</a></p>
</li>
<li><p><a href="https://github.com/ClaudeCoulombe/GBC_book_DeepLearningBook" target="_blank" rel="noopener">https://github.com/ClaudeCoulombe/GBC_book_DeepLearningBook</a></p>
</li>
<li><p><a href="https://en.wikipedia.org/wiki/Feedforward_neural_network" target="_blank" rel="noopener">wiki - Feedforward neural network</a></p>
</li>
<li><p><a href="https://zhuanlan.zhihu.com/p/21407711" target="_blank" rel="noopener">backpropagation</a></p>
</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Monad Kai</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="../../../../onlookerliu.github.io/2018/04/13/ML-Lectures-Perceptron/">onlookerliu.github.io/2018/04/13/ML-Lectures-Perceptron/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="onlookerliu.github.io" target="_blank">Code@浮生记</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="../../../../tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="../../../../tags/deep-learning/">deep learning</a><a class="post-meta__tags" href="../../../../tags/python/">python</a></div><div class="post-qr-code"><div class="post-qr-code-item"><img class="post-qr-code__img" src="http://oxyywddt8.bkt.clouddn.com/qrcode/ali-qrcode.jpg"><div class="post-qr-code__desc">支付宝打赏</div></div><div class="post-qr-code-item"><img class="post-qr-code__img" src="http://oxyywddt8.bkt.clouddn.com/qrcode/Wechat.jpeg"><div class="post-qr-code__desc">微信打赏</div></div></div><div class="addthis_inline_share_toolbox pull-right"></div><script src="//s7.addthis.com/js/300/addthis_widget.js#pubid=undefined" async></script><nav id="pagination"><div class="prev-post pull-left"><a href="../../14/LeetCode-Notes-026/"><i class="fa fa-chevron-left">  </i><span>LeetCode Notes 026</span></a></div><div class="next-post pull-right"><a href="../Project-Euler-029/"><span>Project-Euler-029</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="disqus_thread"></div><script>var unused = null;
var disqus_config = function () {
  this.page.url = 'onlookerliu.github.io/2018/04/13/ML-Lectures-Perceptron/';
  this.page.identifier = '2018/04/13/ML-Lectures-Perceptron/';
  this.page.title = 'ML-Lectures Perceptron';
}
var d = document, s = d.createElement('script');
s.src = "https://" + 'onlookerliu' +".disqus.com/embed.js";
s.setAttribute('data-timestamp', '' + +new Date());
(d.head || d.body).appendChild(s);</script><script id="dsq-count-src" src="https://onlookerliu.disqus.com/count.js" async></script></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2017 - 2018 By Monad Kai</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="../../../../js/third-party/anime.min.js"></script><script src="../../../../js/third-party/jquery.min.js"></script><script src="../../../../js/third-party/jquery.fancybox.min.js"></script><script src="../../../../js/third-party/velocity.min.js"></script><script src="../../../../js/third-party/velocity.ui.min.js"></script><script src="../../../../js/utils.js?version=1.5.3"></script><script src="../../../../js/fancybox.js?version=1.5.3"></script><script src="../../../../js/sidebar.js?version=1.5.3"></script><script src="../../../../js/copy.js?version=1.5.3"></script><script src="../../../../js/fireworks.js?version=1.5.3"></script><script src="../../../../js/transition.js?version=1.5.3"></script><script src="../../../../js/scroll.js?version=1.5.3"></script><script src="../../../../js/head.js?version=1.5.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script></body></html>